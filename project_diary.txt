#%% raw

Dataset side
* add apropirate settings for TFRecordDataset -V
* cache (before transformations) just to avoid disk access (should be ~9gigs) -V
* move cache to ramfs -V
* add augmentation ,after cache, probably before transformations
* add prefetch, shuffle -V
* create validation dataset -V

Compilation side
* callbacks:
    *tensorboard -V
    *saving weights every epoch -V
    *learning rate -V
    *
* add tensorboard callback -V

* add hyper parameters (learning rate, learning rate decay)
* Figure out metrics (accuracy)
    *add recall ,mean absoulte error, and 'island' recall error -V
    *sort metrics by stage -V
* add validation -V

All
* add comments

TPUs
* try with TPUs -VVV

*Project
add augmentation
add regularization (dropout)

Dangling layers
#solve that issue -V

Results
*caching seems to eat only gpu memory
*Intersting to try a ramfs or a BytesIO object , (ramfs is probably better though as tf handles the interface)
*This should allow to increase batch size
-Full epoc currently is 1:15h
TPUs
-After figuring out TPUs, 13min epoch is achieved!

-execution adjustments, batch siez, caching.
-

Need to figure out what's going on with the PAF stages, all losses seem the same.
1.maybe the same loss function is applied to all.
check names of stages, and maybe have different losses for each.
check model outputs and graphs
2.maybe the dataset is all zeros for pafs, or the limbs are too small,
optionally disable feedforward links
3.another option is that it's ok

observations
*when the gradients explode, the PAF stages losses are very different:
loss: 6034892169562175.0000 - stage1paf_output_loss: 2.7209 - stage2paf_output_loss: 3066.9797 - stage3paf_output_loss: 1367401.7500 - stage4paf_output_loss: 648865408.0000

explosing gradients seem to have something with reloading checkpoints.
if true I should open an issue, on econd look it might just be the learning rate.

Learning rate
0.0001 works fine
0.003 explodes
0.002 is too much as well


After training, the PAF stages dont seem to train at all. the converge to a zero minima.\n",
to solve the issue two solutions are implmented.\n",
1.Implement the masking feature in the loss function, for the data in COCO dataset\n",
2.Instead of using the paper's approach of hard areas for the PAFs, using a gaussian distribution for the lengths of the joints.\n",
3.Adding a start and end gaussian distrubtion to the joint vectors (the beginning and end of the joints), this should improve handling of smaller joints like nose-eye and eye-ear.\n",
4.Adding a neck kpt and changing the nose connection to this point. this reflection of anatomy should work better."


ERROR,
loss is NaN after 5 batch (128) training.
-lowering the LR didn't help
-switching to batch size one to find if is related to data.
-running 1 batch size, the training is going on fine, before crashing.
-error occurs at item 815., disabled shuffle
-retry, happens at 795
-interesting bug,
-fixed, problem was zero length joints

Switching to double branching model