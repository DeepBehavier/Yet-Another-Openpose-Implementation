{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#TODO\n",
    "Dataset side\n",
    "* add apropirate settings for TFRecordDataset -V\n",
    "* cache (before transformations) just to avoid disk access (should be ~9gigs) -V\n",
    "* move cache to ramfs -V\n",
    "* add augmentation ,after cache, probably before transformations \n",
    "* add prefetch, shuffle -V \n",
    "* create validation dataset -V\n",
    "\n",
    "Compilation side\n",
    "* callbacks:\n",
    "    *tensorboard -V\n",
    "    *saving weights every epoch -V\n",
    "    *learning rate -V\n",
    "    *\n",
    "* add tensorboard callback -V\n",
    "\n",
    "* add hyper parameters (learning rate, learning rate decay)\n",
    "* Figure out metrics (accuracy)\n",
    "    *add recall ,mean absoulte error, and 'island' recall error -V\n",
    "    *sort metrics by stage -V\n",
    "* add validation -V\n",
    "\n",
    "All\n",
    "* add comments\n",
    "\n",
    "TPUs\n",
    "* try with TPUs -VVV\n",
    "\n",
    "*Project\n",
    "add augmentation\n",
    "add regularization (dropout)\n",
    "\n",
    "Dangling layers\n",
    "#solve that issue -V\n",
    "\n",
    "Results\n",
    "*caching seems to eat only gpu memory\n",
    "*Intersting to try a ramfs or a BytesIO object , (ramfs is probably better though as tf handles the interface)\n",
    "*This should allow to increase batch size\n",
    "-Full epoc currently is 1:15h\n",
    "TPUs\n",
    "-After figuring out TPUs, 13min epoch is achieved!\n",
    "\n",
    "-execution adjustments, batch siez, caching.\n",
    "-\n",
    "\n",
    "Need to figure out what's going on with the PAF stages, all losses seem the same.\n",
    "1.maybe the same loss function is applied to all.\n",
    "check names of stages, and maybe have different losses for each.\n",
    "check model outputs and graphs\n",
    "2.maybe the dataset is all zeros for pafs, or the limbs are too small,\n",
    "optionally disable feedforward links\n",
    "3.another option is that it's ok\n",
    "\n",
    "observations\n",
    "*when the gradients explode, the PAF stages losses are very different:\n",
    "loss: 6034892169562175.0000 - stage1paf_output_loss: 2.7209 - stage2paf_output_loss: 3066.9797 - stage3paf_output_loss: 1367401.7500 - stage4paf_output_loss: 648865408.0000\n",
    "\n",
    "explosing gradients seem to have something with reloading checkpoints.\n",
    "if true I should open an issue, on econd look it might just be the learning rate.\n",
    "\n",
    "Learning rate\n",
    "0.0001 works fine \n",
    "0.003 explodes\n",
    "0.002 is too much as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "michael_zl_prime\n",
      "                  Credentialed Accounts\n",
      "ACTIVE  ACCOUNT\n",
      "*       312164605303-compute@developer.gserviceaccount.com\n",
      "\n",
      "To set the active account, run:\n",
      "    $ gcloud config set account `ACCOUNT`\n",
      "\n",
      "Python running from: /usr\n",
      "Current working dir /home/michael_zl_prime/project_drive/project\n"
     ]
    }
   ],
   "source": [
    "#verify user\n",
    "!whoami\n",
    "#verify user account, if running on google cloud, otherwise ignore\n",
    "!gcloud auth list\n",
    "#which enciorment/virtualenv running in\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"Python running from:\",sys.prefix)\n",
    "print(\"Current working dir\",os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#start tensor board\n",
    "# must run \n",
    "#/usr/local/bin/tensorboard serve --logdir gs://dl_training_results/tensorboard --port 8889 --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving TFrecords in TPU_mode\n",
      "Tensorflow version: 2.1.0-dev20191124\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "import dataset_functions\n",
    "from models.six_stage_linear_model import ModelMaker\n",
    "import callbacks\n",
    "import dataset_builder\n",
    "from config import *\n",
    "import load_weights\n",
    "\n",
    "print(\"Tensorflow version:\",tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#override config\n",
    "BATCH_SIZE=128\n",
    "BASE_LEARNING_RATE=0.001\n",
    "LEARNING_RATE_SCHEDUELE=np.zeros(1000)\n",
    "LEARNING_RATE_SCHEDUELE[:3]=0.2\n",
    "LEARNING_RATE_SCHEDUELE[3:20]=1\n",
    "LEARNING_RATE_SCHEDUELE[20:40]=1\n",
    "LEARNING_RATE_SCHEDUELE[40:100]=1\n",
    "LEARNING_RATE_SCHEDUELE[100:]=0.3\n",
    "LEARNING_RATE_SCHEDUELE*=BASE_LEARNING_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results bucket connectivity\n",
      "Copying file:///tmp/test [Content-Type=application/octet-stream]...\n",
      "/ [1 files][    0.0 B/    0.0 B]                                                \n",
      "Operation completed over 1 objects.                                              \n",
      "Removing gs://dl_training_results/tensorboard/test...\n",
      "/ [1 objects]                                                                   \n",
      "Operation completed over 1 objects.                                              \n",
      "Copying file:///tmp/test [Content-Type=application/octet-stream]...\n",
      "/ [1 files][    0.0 B/    0.0 B]                                                \n",
      "Operation completed over 1 objects.                                              \n",
      "Removing gs://dl_training_results/checkpoints/test...\n",
      "/ [1 objects]                                                                   \n",
      "Operation completed over 1 objects.                                              \n",
      "Testing dataset bucket connectivity\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-001.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-002.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-003.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-004.tfrecords\n",
      "Testing TPU connectivity\n",
      "\n",
      "Starting Nmap 7.40 ( https://nmap.org ) at 2019-12-02 14:04 UTC\n",
      "Nmap scan report for 10.0.3.2\n",
      "Host is up (0.0015s latency).\n",
      "PORT     STATE SERVICE\n",
      "8470/tcp open  cisco-avp\n",
      "\n",
      "Nmap done: 1 IP address (1 host up) scanned in 0.04 seconds\n",
      "Trying to connect to a TPU node\n",
      "\n",
      "!!!MAKE SURE THE TPU ADDRESS IS CORRECT!!\n",
      "1.ip must be correct\n",
      "2.tpu must be turned on\n",
      "3.version must be 'nightly-2.x'\n",
      "4.tpu must be reachable (check with gce netowrking/connectivity test)\n",
      "if not this will hang!\n",
      "\n",
      "Trying to connect to: grpc://10.0.3.2:8470\n",
      "INFO:tensorflow:Initializing the TPU system: 10.0.3.2:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: 10.0.3.2:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "if TPU_MODE:\n",
    "    #import tpu_training.test_connectivity\n",
    "    #tpu_training.test_connectivity.test_connectivity()\n",
    "    print(\"Testing results bucket connectivity\")\n",
    "    !touch /tmp/test\n",
    "    !gsutil cp /tmp/test {TENSORBOARD_PATH}/test\n",
    "    !gsutil rm {TENSORBOARD_PATH}/test\n",
    "    !gsutil cp /tmp/test {CHECKPOINTS_PATH}/test\n",
    "    !gsutil rm {CHECKPOINTS_PATH}/test\n",
    "    print(\"Testing dataset bucket connectivity\")\n",
    "    !gsutil ls gs://{GCS_TFRECORDS_BUCKETNAME} | head -4\n",
    "    print(\"Testing TPU connectivity\")\n",
    "    !nmap -Pn -p8470 {TPU_IP}\n",
    "    import tpu_training.init_tpu as init_tpu\n",
    "    strategy,resolver=init_tpu.init_tpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.config.experimental_connect_to_cluster(resolver)\n",
    "#tpu_topo=tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "#tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.distribute.tpu_strategy.TPUStrategy at 0x7f795a2cb128>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "importlib.reload(dataset_functions)\n",
    "TF_parser = dataset_functions.TFrecordParser()\n",
    "\n",
    "tfrecord_files_train,tfrecord_files_val=dataset_builder.get_tfrecord_filenames()\n",
    "def build_training_ds(tfrecord_filenames:list)->tf.data.Dataset:\n",
    "    \"\"\"Generate training dataset from TFrecord file locations\n",
    "    :param tfrecord_files should be list of correct TFrecord filename, either local or remote (gcs, with gs:// prefix)\"\"\"\n",
    "    # TFrecord files to raw format\n",
    "    ds = tf.data.TFRecordDataset(tfrecord_filenames)  # numf reads can be put here, but I don't think I/O is the bottleneck\n",
    "\n",
    "    # raw format to imgs,tensors(coords kpts)\n",
    "    ds = ds.map(TF_parser.read_tfrecord)\n",
    "\n",
    "    # cache  ,caching is here before decompressing jpgs and label tensors (should be ~9GB) , (full dataset should be ~90, cache later if RAM aviable)\n",
    "    if CACHE: ds = ds.cache()\n",
    "    if SHUFFLE: ds = ds.shuffle(100)\n",
    "\n",
    "    # Augmentation should be here, to operate on smaller tensors\n",
    "\n",
    "    # imgs,tensors to label_tensors (46,46,17/38)\n",
    "    ds = ds.map(dataset_functions.make_label_tensors)\n",
    "    # imgs,label_tensors arrange for model outputs\n",
    "    ds = ds.map(dataset_functions.place_training_labels)\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.repeat()\n",
    "    if PREFETCH: ds = ds.prefetch(PREFETCH)\n",
    "    return ds\n",
    "dst=build_training_ds(tfrecord_files_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following TFrecords:\n",
      " gs://datasets_bucket_a/person_keypoints_train2017-001.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-002.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-003.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-004.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-005.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-006.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-007.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-008.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-009.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-010.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-011.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-012.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-013.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-014.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-015.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-016.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-017.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-018.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-019.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-020.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-021.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-022.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-023.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-024.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-025.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-026.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-027.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-028.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-029.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-030.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-031.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-032.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-033.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-034.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-035.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-036.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-037.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-038.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-039.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-040.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-041.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-042.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-043.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-044.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-045.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-046.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-047.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-048.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-049.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-050.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-051.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-052.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-053.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-054.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-055.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-056.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_train2017-057.tfrecords \n",
      " gs://datasets_bucket_a/person_keypoints_val2017-001.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_val2017-002.tfrecords\n",
      "gs://datasets_bucket_a/person_keypoints_val2017-003.tfrecords\n",
      "Building training dataset\n",
      "Training dataset shape: <PrefetchDataset shapes: ((None, 368, 368, 3), ((None, 46, 46, 38), (None, 46, 46, 38), (None, 46, 46, 38), (None, 46, 46, 38), (None, 46, 46, 17), (None, 46, 46, 17))), types: (tf.float32, (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32))>\n",
      "Building validation dataset\n",
      "Validation dataset shape: <BatchDataset shapes: ((None, 368, 368, 3), ((None, 46, 46, 38), (None, 46, 46, 38), (None, 46, 46, 38), (None, 46, 46, 38), (None, 46, 46, 17), (None, 46, 46, 17))), types: (tf.float32, (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32))>\n"
     ]
    }
   ],
   "source": [
    "tfrecord_files_train,tfrecord_files_val=dataset_builder.get_tfrecord_filenames()\n",
    "print(\"Found the following TFrecords:\\n\",\"\\n\".join(tfrecord_files_train),\"\\n\", \"\\n\".join(tfrecord_files_val))\n",
    "\n",
    "print(\"Building training dataset\")\n",
    "dst=dataset_builder.build_training_ds(tfrecord_files_train)\n",
    "print(\"Training dataset shape:\",dst)\n",
    "print(\"Building validation dataset\")\n",
    "dsv=dataset_builder.build_validation_ds(tfrecord_files_val)\n",
    "print(\"Validation dataset shape:\",dsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Test element\n",
    "# dst_iter=iter(dst)\n",
    "# sample_elem=next(dst_iter)\n",
    "# print(\"Dataset shape:\",dst) #this should match the model input, and output stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fit_callbacks=[\n",
    "    callbacks.checkpoints_callback\n",
    "    ,callbacks.tensorboard_callback\n",
    "    ,callbacks.learning_rate_scheduler_callback\n",
    "    ,callbacks.print_lr_callback\n",
    "    ,tf.keras.callbacks.TerminateOnNaN()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "#run to clean tensorboard results\n",
    "#!gsutil -m rm -r {TENSORBOARD_PATH}/*\n",
    "#!gsutil -m rm -r {CHECKPOINTS_PATH}/*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model\n",
    "## Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint=None\n",
    "starting_epoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found these checkpoints\n",
      "0.Dont load checkpoint\n",
      "1 .gs://dl_training_results/checkpoints//02Mon1219-1231/Checkpoint-E0001.ckpt\n",
      "2 .gs://dl_training_results/checkpoints//02Mon1219-1231/Checkpoint-E0002.ckpt\n",
      "3 .gs://dl_training_results/checkpoints//02Mon1219-1231/Checkpoint-E0003.ckpt\n",
      "4 .gs://dl_training_results/checkpoints//02Mon1219-1231/Checkpoint-E0004.ckpt\n",
      "5 .gs://dl_training_results/checkpoints/02Mon1219-1345/Checkpoint-E0001.ckpt\n",
      "6 .gs://dl_training_results/checkpoints/02Mon1219-1348/Checkpoint-E0001.ckpt\n",
      "Please select checkpoint, or 0 to continue without loading0\n"
     ]
    }
   ],
   "source": [
    "checkpoint,starting_epoch=load_weights.checkpoints_prompt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model_maker=ModelMaker() #must be outside scope to keep the graph clean\n",
    "tf.keras.backend.clear_session() #to clean to backaend from the imported model\n",
    "def define():\n",
    "    train_model,test_model=model_maker.create_models()\n",
    "    \n",
    "    #this must match the model output order\n",
    "    metrics={'stage1paf_output':  [tf.keras.metrics.MeanAbsoluteError()]\n",
    "         ,'stage2paf_output':  [tf.keras.metrics.MeanAbsoluteError()]\n",
    "         ,'stage3paf_output':  [tf.keras.metrics.MeanAbsoluteError()]\n",
    "         ,'stage4paf_output':  [tf.keras.metrics.MeanAbsoluteError()]\n",
    "         ,'stage5heatmap_output': [\n",
    "             #AnalogRecall(),\n",
    "             tf.keras.metrics.MeanAbsoluteError()]    \n",
    "         ,'stage6heatmap_output': [\n",
    "             #AnalogRecall(),\n",
    "             tf.keras.metrics.MeanAbsoluteError()]\n",
    "        }\n",
    "    \n",
    "    train_model.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(BASE_LEARNING_RATE)\n",
    "                    ,loss=tf.keras.losses.MeanSquaredError(name=\"MSE_loss\")\n",
    "                    ,metrics=metrics                    \n",
    "                   )\n",
    "    return train_model,test_model\n",
    "\n",
    "if TPU_MODE:\n",
    "    with strategy.scope():\n",
    "        train_model,test_model=define()\n",
    "        if (checkpoint):\n",
    "            train_model.load_weights(checkpoint)\n",
    "else:\n",
    "    train_model,test_model=define()\n",
    "    if (checkpoint):\n",
    "        train_model.load_weights(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Actually training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 437 steps\n",
      "\n",
      "Learning rate for epoch 0 is 0.00019999999494757503\n",
      "Epoch 1/100\n",
      "  1/437 [..............................] - ETA: 8:04:40 - loss: 0.2510 - stage1paf_output_loss: 0.0305 - stage2paf_output_loss: 0.0200 - stage3paf_output_loss: 0.0203 - stage4paf_output_loss: 0.0258 - stage5heatmap_output_loss: 0.0888 - stage6heatmap_output_loss: 0.0656 - stage1paf_output_mean_absolute_error: 0.0872 - stage2paf_output_mean_absolute_error: 0.0754 - stage3paf_output_mean_absolute_error: 0.0812 - stage4paf_output_mean_absolute_error: 0.0755 - stage5heatmap_output_mean_absolute_error: 0.1663 - stage6heatmap_output_mean_absolute_error: 0.1711WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.007338). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.007338). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/437 [=========>....................] - ETA: 6:18 - loss: 0.0801 - stage1paf_output_loss: 0.0027 - stage2paf_output_loss: 0.0026 - stage3paf_output_loss: 0.0026 - stage4paf_output_loss: 0.0027 - stage5heatmap_output_loss: 0.0342 - stage6heatmap_output_loss: 0.0353 - stage1paf_output_mean_absolute_error: 0.0039 - stage2paf_output_mean_absolute_error: 0.0039 - stage3paf_output_mean_absolute_error: 0.0041 - stage4paf_output_mean_absolute_error: 0.0040 - stage5heatmap_output_mean_absolute_error: 0.0947 - stage6heatmap_output_mean_absolute_error: 0.0942"
     ]
    }
   ],
   "source": [
    "train_history=train_model.fit(\n",
    "    dst\n",
    "    ,epochs=TRAINING_EPOCHS\n",
    "    ,steps_per_epoch=STEPS_PER_EPOCH\n",
    "    ,validation_data=dsv\n",
    "    ,callbacks=fit_callbacks\n",
    "    ,initial_epoch=starting_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model_path=MODELS_PATH+datetime.datetime.now().strftime(\"%d(%a)%m%y-%H%M\")\n",
    "tf.keras.models.save_model(train_model,model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#v.show_pafs_kpts_img(img.numpy(),paf.numpy(),kpt.numpy(),1,1) #can be used to draw the tensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
