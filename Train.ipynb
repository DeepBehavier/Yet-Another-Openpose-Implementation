{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Yet Another OpenPose "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "michael_zl_prime\n",
      "                  Credentialed Accounts\n",
      "ACTIVE  ACCOUNT\n",
      "        312164605303-compute@developer.gserviceaccount.com\n",
      "*       michael.zl.prime@gmail.com\n",
      "\n",
      "To set the active account, run:\n",
      "    $ gcloud config set account `ACCOUNT`\n",
      "\n",
      "Python running from: /usr\n",
      "Current working dir /home/michael_zl_prime/project_drive/project\n"
     ]
    }
   ],
   "source": [
    "#verify user\n",
    "!whoami\n",
    "#verify user account, if running on google cloud, otherwise ignore\n",
    "!gcloud auth list\n",
    "#which Environment/virtualenv running in\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"Python running from:\",sys.prefix)\n",
    "print(\"Current working dir\",os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#start tensor board\n",
    "# must run \n",
    "#/usr/local/bin/tensorboard serve --logdir gs://dl_training_results/tensorboard --port 8889 --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.compute.tpus.start) INVALID_ARGUMENT: cannot start an already running instance\r\n"
     ]
    }
   ],
   "source": [
    "#start TPU\n",
    "!gcloud compute tpus start node-1 --zone us-central1-c\n",
    "#stop TPU\n",
    "#!gcloud compute tpus stop node-1 --zone us-central1-c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import default_config as cfg\n",
    "\n",
    "#import local_storage_config as storage_cfg\n",
    "import remote_storage_config as storage_cfg\n",
    "cfg.__dict__.update(storage_cfg.__dict__)\n",
    "\n",
    "import tpu_training.TPU_config as TPU_config\n",
    "cfg.__dict__.update(TPU_config.__dict__) #comment out to disable TPU training\n",
    "cfg.RUN_NAME=\"Full_model_2_2augs_dp01_15joint_nobn-\" #for reference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.1.0-dev20191203\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow version:\",tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results bucket connectivity\n",
      "Copying file:///tmp/test [Content-Type=application/octet-stream]...\n",
      "/ [1 files][    0.0 B/    0.0 B]                                                \n",
      "Operation completed over 1 objects.                                              \n",
      "Removing gs://dl_training_results/tensorboard/test...\n",
      "/ [1 objects]                                                                   \n",
      "Operation completed over 1 objects.                                              \n",
      "Copying file:///tmp/test [Content-Type=application/octet-stream]...\n",
      "/ [1 files][    0.0 B/    0.0 B]                                                \n",
      "Operation completed over 1 objects.                                              \n",
      "Removing gs://dl_training_results/checkpoints/test...\n",
      "/ [1 objects]                                                                   \n",
      "Operation completed over 1 objects.                                              \n",
      "Testing dataset bucket connectivity\n",
      "gs://datasets_bucket_a/training-001.tfrecords\n",
      "gs://datasets_bucket_a/training-002.tfrecords\n",
      "gs://datasets_bucket_a/training-003.tfrecords\n",
      "gs://datasets_bucket_a/training-004.tfrecords\n",
      "Testing TPU connectivity\n",
      "\n",
      "Starting Nmap 7.40 ( https://nmap.org ) at 2019-12-11 20:50 UTC\n",
      "Nmap scan report for 10.0.3.2\n",
      "Host is up (0.0014s latency).\n",
      "PORT     STATE SERVICE\n",
      "8470/tcp open  cisco-avp\n",
      "\n",
      "Nmap done: 1 IP address (1 host up) scanned in 0.04 seconds\n",
      "Trying to connect to a TPU node\n",
      "\n",
      "!!!MAKE SURE THE TPU ADDRESS IS CORRECT!!\n",
      "1.ip must be correct\n",
      "2.tpu must be turned on\n",
      "3.version must be 'nightly-2.x'\n",
      "4.tpu must be reachable (check with gce netowrking/connectivity test)\n",
      "if not this will hang!\n",
      "\n",
      "Trying to connect to: grpc://10.0.3.2:8470\n",
      "INFO:tensorflow:Initializing the TPU system: grpc://10.0.3.2:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.0.3.2:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "if cfg.TPU_MODE:\n",
    "    print(\"Testing results bucket connectivity\")\n",
    "    !touch /tmp/test\n",
    "    !gsutil cp /tmp/test {cfg.TENSORBOARD_PATH}/test\n",
    "    !gsutil rm {cfg.TENSORBOARD_PATH}/test\n",
    "    !gsutil cp /tmp/test {cfg.CHECKPOINTS_PATH}/test\n",
    "    !gsutil rm {cfg.CHECKPOINTS_PATH}/test\n",
    "    print(\"Testing dataset bucket connectivity\")\n",
    "    !gsutil ls gs://{cfg.GCS_TFRECORDS_BUCKETNAME} | head -4\n",
    "    print(\"Testing TPU connectivity\")\n",
    "    !nmap -Pn -p8470 {cfg.TPU_IP}\n",
    "    import tpu_training.init_TPU as init_TPU\n",
    "    strategy,resolver=init_TPU.init_tpu() #This must be run before any imports!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import dataset_functions\n",
    "import models.six_stage_linear_model as model\n",
    "import callbacks\n",
    "import dataset_builder\n",
    "import load_weights\n",
    "import loss_metrics\n",
    "from utils import now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# import importlib as il\n",
    "# il.reload() #useful to reload any changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving TFrecords from: gs://datasets_bucket_a/training\n",
      "Retrieving TFrecords from: gs://datasets_bucket_a/validation\n",
      "Found the following training TFrecords:\n",
      " gs://datasets_bucket_a/training-001.tfrecords\n",
      "gs://datasets_bucket_a/training-002.tfrecords\n",
      "gs://datasets_bucket_a/training-003.tfrecords\n",
      "gs://datasets_bucket_a/training-004.tfrecords\n",
      "gs://datasets_bucket_a/training-005.tfrecords\n",
      "gs://datasets_bucket_a/training-006.tfrecords\n",
      "gs://datasets_bucket_a/training-007.tfrecords\n",
      "gs://datasets_bucket_a/training-008.tfrecords\n",
      "gs://datasets_bucket_a/training-009.tfrecords\n",
      "gs://datasets_bucket_a/training-010.tfrecords\n",
      "gs://datasets_bucket_a/training-011.tfrecords\n",
      "gs://datasets_bucket_a/training-012.tfrecords\n",
      "gs://datasets_bucket_a/training-013.tfrecords\n",
      "gs://datasets_bucket_a/training-014.tfrecords\n",
      "gs://datasets_bucket_a/training-015.tfrecords\n",
      "gs://datasets_bucket_a/training-016.tfrecords\n",
      "gs://datasets_bucket_a/training-017.tfrecords\n",
      "gs://datasets_bucket_a/training-018.tfrecords\n",
      "gs://datasets_bucket_a/training-019.tfrecords\n",
      "gs://datasets_bucket_a/training-020.tfrecords\n",
      "gs://datasets_bucket_a/training-021.tfrecords\n",
      "gs://datasets_bucket_a/training-022.tfrecords\n",
      "gs://datasets_bucket_a/training-023.tfrecords\n",
      "gs://datasets_bucket_a/training-024.tfrecords\n",
      "gs://datasets_bucket_a/training-025.tfrecords\n",
      "gs://datasets_bucket_a/training-026.tfrecords\n",
      "gs://datasets_bucket_a/training-027.tfrecords\n",
      "gs://datasets_bucket_a/training-028.tfrecords\n",
      "gs://datasets_bucket_a/training-029.tfrecords\n",
      "gs://datasets_bucket_a/training-030.tfrecords\n",
      "gs://datasets_bucket_a/training-031.tfrecords\n",
      "gs://datasets_bucket_a/training-032.tfrecords\n",
      "gs://datasets_bucket_a/training-033.tfrecords\n",
      "gs://datasets_bucket_a/training-034.tfrecords\n",
      "gs://datasets_bucket_a/training-035.tfrecords\n",
      "gs://datasets_bucket_a/training-036.tfrecords\n",
      "gs://datasets_bucket_a/training-037.tfrecords\n",
      "gs://datasets_bucket_a/training-038.tfrecords\n",
      "gs://datasets_bucket_a/training-039.tfrecords\n",
      "gs://datasets_bucket_a/training-040.tfrecords\n",
      "gs://datasets_bucket_a/training-041.tfrecords\n",
      "gs://datasets_bucket_a/training-042.tfrecords\n",
      "gs://datasets_bucket_a/training-043.tfrecords\n",
      "gs://datasets_bucket_a/training-044.tfrecords\n",
      "gs://datasets_bucket_a/training-045.tfrecords\n",
      "gs://datasets_bucket_a/training-046.tfrecords\n",
      "gs://datasets_bucket_a/training-047.tfrecords\n",
      "gs://datasets_bucket_a/training-048.tfrecords\n",
      "gs://datasets_bucket_a/training-049.tfrecords\n",
      "gs://datasets_bucket_a/training-050.tfrecords\n",
      "gs://datasets_bucket_a/training-051.tfrecords\n",
      "gs://datasets_bucket_a/training-052.tfrecords\n",
      "gs://datasets_bucket_a/training-053.tfrecords\n",
      "gs://datasets_bucket_a/training-054.tfrecords\n",
      "gs://datasets_bucket_a/training-055.tfrecords\n",
      "gs://datasets_bucket_a/training-056.tfrecords\n",
      "gs://datasets_bucket_a/training-057.tfrecords\n",
      "Found the following validation TFrecords:\n",
      " gs://datasets_bucket_a/validation-001.tfrecords\n",
      "gs://datasets_bucket_a/validation-002.tfrecords\n",
      "gs://datasets_bucket_a/validation-003.tfrecords\n",
      "Building training dataset\n",
      "Training dataset shape: <PrefetchDataset shapes: (((None, 368, 368, 3), (None, 46, 46, 1)), ((None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 19), (None, 46, 46, 19))), types: ((tf.float32, tf.float32), (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32))>\n",
      "Building validation dataset\n",
      "Validation dataset shape: <BatchDataset shapes: (((None, 368, 368, 3), (None, 46, 46, 1)), ((None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 19), (None, 46, 46, 19))), types: ((tf.float32, tf.float32), (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32))>\n"
     ]
    }
   ],
   "source": [
    "model_ds=model.ModelDatasetComponent(cfg)\n",
    "\n",
    "tfrecord_files_train=dataset_builder.get_tfrecord_filenames(cfg.TRAIN_TFRECORDS,cfg)\n",
    "tfrecord_files_valid=dataset_builder.get_tfrecord_filenames(cfg.VALID_TFRECORDS,cfg)\n",
    "print(\"Found the following training TFrecords:\\n\",\"\\n\".join(tfrecord_files_train))\n",
    "print(\"Found the following validation TFrecords:\\n\",\"\\n\".join(tfrecord_files_valid))\n",
    "\n",
    "print(\"Building training dataset\")\n",
    "dst=dataset_builder.build_training_ds(tfrecord_files_train,model_ds.place_training_labels,cfg)\n",
    "print(\"Training dataset shape:\",dst)\n",
    "print(\"Building validation dataset\")\n",
    "dsv=dataset_builder.build_validation_ds(tfrecord_files_valid,model_ds.place_training_labels,cfg)\n",
    "print(\"Validation dataset shape:\",dsv)\n",
    "STEPS_PER_EPOCH = int(cfg.DATASET_SIZE / cfg.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Test element\n",
    "# dst_iter=iter(dst)\n",
    "# sample_elem=next(dst_iter)\n",
    "# print(\"Dataset shape:\",dst) #this should match the model input, and output stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#testing mask\n",
    "# sample_elem=next(dst_iter)\n",
    "# m=sample_elem[1][1][0,...,0]\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(m)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model\n",
    "### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if cfg.ASK_FOR_CHECKPOINTS:\n",
    "    checkpoint,starting_epoch=load_weights.checkpoints_prompt(cfg)\n",
    "else:\n",
    "    checkpoint=None\n",
    "    starting_epoch=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_maker=model.ModelMaker(cfg) #must be outside scope to keep the graph clean\n",
    "tf.keras.backend.clear_session() #to clean to backaend from the imported model\n",
    "def define():\n",
    "    train_model,test_model=model_maker.create_models()\n",
    "    \n",
    "#     if cfg.INCLUDE_MASK:\n",
    "#         losses=[loss_metrics.MaskedMeanAbsoluteError()\n",
    "#                 ,loss_metrics.MaskedMeanAbsoluteError()\n",
    "#                 ,loss_metrics.MaskedMeanAbsoluteError()\n",
    "#                 ,loss_metrics.MaskedMeanAbsoluteError()\n",
    "#                 ,loss_metrics.MaskedMeanSquaredError()\n",
    "#                 ,loss_metrics.MaskedMeanSquaredError()]\n",
    "                      \n",
    "#     else:\n",
    "#         raise NotImplementedError       \n",
    "    \n",
    "    #this must match the model output order\n",
    "#     metrics=[\n",
    "#           []\n",
    "#          ,[]\n",
    "#          ,[]\n",
    "#          ,[]\n",
    "#          ,[tf.keras.metrics.MeanAbsoluteError()]    \n",
    "#          ,[tf.keras.metrics.MeanAbsoluteError()]\n",
    "#         ]\n",
    "    \n",
    "    train_model.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(cfg.BASE_LEARNING_RATE)                   \n",
    "                    ,loss=loss_metrics.MaskedMeanSquaredError()\n",
    "                    ,loss_weights=[10,10,10,10,0.2,0.2]\n",
    "                    #,metrics=metrics                           \n",
    "                   )\n",
    "    return train_model,test_model\n",
    "\n",
    "if cfg.TPU_MODE:\n",
    "    with strategy.scope():\n",
    "        train_model,test_model=define()\n",
    "        if (checkpoint):\n",
    "            train_model.load_weights(checkpoint)\n",
    "else:\n",
    "    train_model,test_model=define()\n",
    "    if (checkpoint):\n",
    "        train_model.load_weights(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_callbacks=[\n",
    "    callbacks.make_checkpoint_callback(cfg)\n",
    "    ,callbacks.make_tensorboard_callback(cfg)\n",
    "    ,callbacks.make_LRscheduler_callback(cfg.LEARNING_RATE_SCHEDUELE)\n",
    "    ,callbacks.PrintLR()\n",
    "    ,tf.keras.callbacks.TerminateOnNaN()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "#run to clean tensorboard results\n",
    "#!gsutil -m rm -r {cfg.TENSORBOARD_PATH}/*\n",
    "#!gsutil -m rm -r {cfg.CHECKPOINTS_PATH}/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training\n",
    "Actually training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 437 steps\n",
      "\n",
      "Learning rate for epoch 0 is 0.0010000000474974513\n",
      "Epoch 1/100\n",
      "  1/437 [..............................] - ETA: 8:49:38 - loss: 2.8115 - s1pafs_output_loss: 0.0524 - s2pafs_output_loss: 0.0859 - s3pafs_output_loss: 0.0687 - s4pafs_output_loss: 0.0717 - s5kpts_output_loss: 0.0589 - s6kpts_output_loss: 0.0655WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.091138). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.091138). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436/437 [============================>.] - ETA: 2s - loss: 0.2338 - s1pafs_output_loss: 0.0050 - s2pafs_output_loss: 0.0057 - s3pafs_output_loss: 0.0056 - s4pafs_output_loss: 0.0057 - s5kpts_output_loss: 0.0364 - s6kpts_output_loss: 0.0373\n",
      "Epoch 00001: saving model to gs://dl_training_results/checkpoints/Full_model_2_2augs_dp01_15joint_nobn-11Wed1219-2050/-E0001.ckpt\n",
      "437/437 [==============================] - 968s 2s/step - loss: 0.2337 - s1pafs_output_loss: 0.0050 - s2pafs_output_loss: 0.0057 - s3pafs_output_loss: 0.0056 - s4pafs_output_loss: 0.0057 - s5kpts_output_loss: 0.0364 - s6kpts_output_loss: 0.0373 - val_loss: 0.2033 - val_s1pafs_output_loss: 0.0047 - val_s2pafs_output_loss: 0.0047 - val_s3pafs_output_loss: 0.0048 - val_s4pafs_output_loss: 0.0047 - val_s5kpts_output_loss: 0.0336 - val_s6kpts_output_loss: 0.0343\n",
      "\n",
      "Learning rate for epoch 1 is 0.0010000000474974513\n",
      "Epoch 2/100\n",
      "436/437 [============================>.] - ETA: 1s - loss: 0.2033 - s1pafs_output_loss: 0.0047 - s2pafs_output_loss: 0.0047 - s3pafs_output_loss: 0.0047 - s4pafs_output_loss: 0.0047 - s5kpts_output_loss: 0.0333 - s6kpts_output_loss: 0.0339\n",
      "Epoch 00002: saving model to gs://dl_training_results/checkpoints/Full_model_2_2augs_dp01_15joint_nobn-11Wed1219-2050/-E0002.ckpt\n",
      "437/437 [==============================] - 783s 2s/step - loss: 0.2033 - s1pafs_output_loss: 0.0047 - s2pafs_output_loss: 0.0047 - s3pafs_output_loss: 0.0047 - s4pafs_output_loss: 0.0047 - s5kpts_output_loss: 0.0333 - s6kpts_output_loss: 0.0339 - val_loss: 0.2009 - val_s1pafs_output_loss: 0.0047 - val_s2pafs_output_loss: 0.0047 - val_s3pafs_output_loss: 0.0047 - val_s4pafs_output_loss: 0.0047 - val_s5kpts_output_loss: 0.0323 - val_s6kpts_output_loss: 0.0317\n",
      "\n",
      "Learning rate for epoch 2 is 0.0010000000474974513\n",
      "Epoch 3/100\n",
      "436/437 [============================>.] - ETA: 1s - loss: 0.1987 - s1pafs_output_loss: 0.0047 - s2pafs_output_loss: 0.0047 - s3pafs_output_loss: 0.0046 - s4pafs_output_loss: 0.0046 - s5kpts_output_loss: 0.0313 - s6kpts_output_loss: 0.0312\n",
      "Epoch 00003: saving model to gs://dl_training_results/checkpoints/Full_model_2_2augs_dp01_15joint_nobn-11Wed1219-2050/-E0003.ckpt\n",
      "437/437 [==============================] - 813s 2s/step - loss: 0.1986 - s1pafs_output_loss: 0.0047 - s2pafs_output_loss: 0.0047 - s3pafs_output_loss: 0.0046 - s4pafs_output_loss: 0.0046 - s5kpts_output_loss: 0.0313 - s6kpts_output_loss: 0.0312 - val_loss: 0.1945 - val_s1pafs_output_loss: 0.0046 - val_s2pafs_output_loss: 0.0046 - val_s3pafs_output_loss: 0.0046 - val_s4pafs_output_loss: 0.0046 - val_s5kpts_output_loss: 0.0285 - val_s6kpts_output_loss: 0.0292\n",
      "\n",
      "Learning rate for epoch 3 is 0.0010000000474974513\n",
      "Epoch 4/100\n",
      "436/437 [============================>.] - ETA: 1s - loss: 0.1931 - s1pafs_output_loss: 0.0046 - s2pafs_output_loss: 0.0045 - s3pafs_output_loss: 0.0045 - s4pafs_output_loss: 0.0045 - s5kpts_output_loss: 0.0283 - s6kpts_output_loss: 0.0287\n",
      "Epoch 00004: saving model to gs://dl_training_results/checkpoints/Full_model_2_2augs_dp01_15joint_nobn-11Wed1219-2050/-E0004.ckpt\n",
      "437/437 [==============================] - 802s 2s/step - loss: 0.1931 - s1pafs_output_loss: 0.0046 - s2pafs_output_loss: 0.0045 - s3pafs_output_loss: 0.0045 - s4pafs_output_loss: 0.0045 - s5kpts_output_loss: 0.0283 - s6kpts_output_loss: 0.0287 - val_loss: 0.1888 - val_s1pafs_output_loss: 0.0045 - val_s2pafs_output_loss: 0.0044 - val_s3pafs_output_loss: 0.0044 - val_s4pafs_output_loss: 0.0044 - val_s5kpts_output_loss: 0.0268 - val_s6kpts_output_loss: 0.0265\n",
      "\n",
      "Learning rate for epoch 4 is 0.0010000000474974513\n",
      "Epoch 5/100\n",
      "191/437 [============>.................] - ETA: 6:41 - loss: 0.1927 - s1pafs_output_loss: 0.0046 - s2pafs_output_loss: 0.0045 - s3pafs_output_loss: 0.0046 - s4pafs_output_loss: 0.0045 - s5kpts_output_loss: 0.0267 - s6kpts_output_loss: 0.0277\n",
      "Epoch 00005: saving model to gs://dl_training_results/checkpoints/Full_model_2_2augs_dp01_15joint_nobn-11Wed1219-2050/-E0005.ckpt\n",
      "191/437 [============>.................] - ETA: 7:06 - loss: 0.1927 - s1pafs_output_loss: 0.0046 - s2pafs_output_loss: 0.0045 - s3pafs_output_loss: 0.0046 - s4pafs_output_loss: 0.0045 - s5kpts_output_loss: 0.0267 - s6kpts_output_loss: 0.0277"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-86cab319a3f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m,\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstarting_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m   \u001b[0mconstant_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[0;34m(tensor, partial)\u001b[0m\n\u001b[1;32m    820\u001b[0m   \"\"\"\n\u001b[1;32m    821\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \"\"\"\n\u001b[1;32m    941\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_history=train_model.fit(\n",
    "    dst\n",
    "    ,epochs=cfg.TRAINING_EPOCHS\n",
    "    ,steps_per_epoch=STEPS_PER_EPOCH\n",
    "    ,validation_data=dsv\n",
    "    ,callbacks=all_callbacks\n",
    "    ,initial_epoch=starting_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1788: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1788: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://dl_training_results/models/test_11Wed1219-2152Full_model_2_2augs_dp01_15joint_nobn-/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://dl_training_results/models/test_11Wed1219-2152Full_model_2_2augs_dp01_15joint_nobn-/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://dl_training_results/models/train_11Wed1219-2152Full_model_2_2augs_dp01_15joint_nobn-/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://dl_training_results/models/train_11Wed1219-2152Full_model_2_2augs_dp01_15joint_nobn-/assets\n"
     ]
    }
   ],
   "source": [
    "tmp_path='gs://dl_training_results/tmp/tensorflow/temp_weights'\n",
    "train_model.save_weights(tmp_path)\n",
    "\n",
    "cpu_train_model,cpu_test_model=define()\n",
    "\n",
    "cpu_train_model.load_weights(tmp_path)\n",
    "cpu_test_model.load_weights(tmp_path)\n",
    "\n",
    "cpu_test_model.save(cfg.MODELS_PATH+\"/test_\"+now()+cfg.RUN_NAME) \n",
    "cpu_train_model.save(cfg.MODELS_PATH+\"/train_\"+now()+cfg.RUN_NAME) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request issued for: [node-1]\n",
      "Waiting for operation [projects/deeplearning-257902/locations/us-central1-c/ope\n",
      "rations/operation-1576101197915-59974a8082483-1301d27e-bbddbb4a] to complete...\n",
      "⠶                                                                              "
     ]
    }
   ],
   "source": [
    "#shut down TPU\n",
    "!gcloud compute tpus stop node-1 --zone us-central1-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "!gcloud compute instances stop instance-1 --zone us-central1-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
