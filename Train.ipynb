{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#TODO\n",
    "Dataset side\n",
    "* add apropirate settings for TFRecordDataset -V\n",
    "* cache (before transformations) just to avoid disk access (should be ~9gigs) -V\n",
    "* move cache to ramfs -V\n",
    "* add augmentation ,after cache, probably before transformations \n",
    "* add prefetch, shuffle -V \n",
    "* create validation dataset -V\n",
    "\n",
    "Compilation side\n",
    "* callbacks:\n",
    "    *tensorboard -V\n",
    "    *saving weights every epoch -V\n",
    "    *learning rate -V\n",
    "    *\n",
    "* add tensorboard callback -V\n",
    "\n",
    "* add hyper parameters (learning rate, learning rate decay)\n",
    "* Figure out metrics (accuracy)\n",
    "    *add recall ,mean absoulte error, and 'island' recall error -V\n",
    "    *sort metrics by stage -V\n",
    "* add validation -V\n",
    "\n",
    "All\n",
    "* add comments\n",
    "\n",
    "TPUs\n",
    "* try with TPUs -VVV\n",
    "\n",
    "*Project\n",
    "add augmentation\n",
    "add regularization (dropout)\n",
    "\n",
    "Dangling layers\n",
    "#solve that issue -V\n",
    "\n",
    "Results\n",
    "*caching seems to eat only gpu memory\n",
    "*Intersting to try a ramfs or a BytesIO object , (ramfs is probably better though as tf handles the interface)\n",
    "*This should allow to increase batch size\n",
    "-Full epoc currently is 1:15h\n",
    "TPUs\n",
    "-After figuring out TPUs, 13min epoch is achieved!\n",
    "\n",
    "-execution adjustments, batch siez, caching.\n",
    "-\n",
    "\n",
    "Need to figure out what's going on with the PAF stages, all losses seem the same.\n",
    "1.maybe the same loss function is applied to all.\n",
    "check names of stages, and maybe have different losses for each.\n",
    "check model outputs and graphs\n",
    "2.maybe the dataset is all zeros for pafs, or the limbs are too small,\n",
    "optionally disable feedforward links\n",
    "3.another option is that it's ok\n",
    "\n",
    "observations\n",
    "*when the gradients explode, the PAF stages losses are very different:\n",
    "loss: 6034892169562175.0000 - stage1paf_output_loss: 2.7209 - stage2paf_output_loss: 3066.9797 - stage3paf_output_loss: 1367401.7500 - stage4paf_output_loss: 648865408.0000\n",
    "\n",
    "explosing gradients seem to have something with reloading checkpoints.\n",
    "if true I should open an issue, on econd look it might just be the learning rate.\n",
    "\n",
    "Learning rate\n",
    "0.0001 works fine \n",
    "0.003 explodes\n",
    "0.002 is too much as well\n",
    "\n",
    "\n",
    "After training, the PAF stages dont seem to train at all. the converge to a zero minima.\\n\",\n",
    "to solve the issue two solutions are implmented.\\n\",\n",
    "1.Implement the masking feature in the loss function, for the data in COCO dataset\\n\",\n",
    "2.Instead of using the paper's approach of hard areas for the PAFs, using a gaussian distribution for the lengths of the joints.\\n\",\n",
    "3.Adding a start and end gaussian distrubtion to the joint vectors (the beginning and end of the joints), this should improve handling of smaller joints like nose-eye and eye-ear.\\n\",\n",
    "4.Adding a neck kpt and changing the nose connection to this point. this reflection of anatomy should work better.\"\n",
    "\n",
    "\n",
    "ERROR,\n",
    "loss is NaN after 5 batch (128) training.\n",
    "-lowering the LR didn't help\n",
    "-switching to batch size one to find if is related to data.\n",
    "-running 1 batch size, the training is going on fine, before crashing.\n",
    "-error occurs at item 815., disabled shuffle\n",
    "-retry, happens at 795\n",
    "-interesting bug, \n",
    "-fixed, problem was zero length joints\n",
    "\n",
    "Switching to double branching model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "michael_zl_prime\n",
      "                  Credentialed Accounts\n",
      "ACTIVE  ACCOUNT\n",
      "        312164605303-compute@developer.gserviceaccount.com\n",
      "*       michael.zl.prime@gmail.com\n",
      "\n",
      "To set the active account, run:\n",
      "    $ gcloud config set account `ACCOUNT`\n",
      "\n",
      "Python running from: /usr\n",
      "Current working dir /home/michael_zl_prime/project_drive/project\n"
     ]
    }
   ],
   "source": [
    "#verify user\n",
    "!whoami\n",
    "#verify user account, if running on google cloud, otherwise ignore\n",
    "!gcloud auth list\n",
    "#which enciorment/virtualenv running in\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"Python running from:\",sys.prefix)\n",
    "print(\"Current working dir\",os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#start tensor board\n",
    "# must run \n",
    "#/usr/local/bin/tensorboard serve --logdir gs://dl_training_results/tensorboard --port 8889 --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.compute.tpus.start) INVALID_ARGUMENT: cannot start an already running instance\r\n"
     ]
    }
   ],
   "source": [
    "#start TPU\n",
    "!gcloud compute tpus start node-1 --zone us-central1-c\n",
    "#stop TPU\n",
    "#!gcloud compute tpus stop node-1 --zone us-central1-c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.1.0-dev20191203\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow version:\",tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results bucket connectivity\n",
      "Copying file:///tmp/test [Content-Type=application/octet-stream]...\n",
      "/ [1 files][    0.0 B/    0.0 B]                                                \n",
      "Operation completed over 1 objects.                                              \n",
      "Removing gs://dl_training_results/tensorboard//test...\n",
      "/ [1 objects]                                                                   \n",
      "Operation completed over 1 objects.                                              \n",
      "Copying file:///tmp/test [Content-Type=application/octet-stream]...\n",
      "/ [1 files][    0.0 B/    0.0 B]                                                \n",
      "Operation completed over 1 objects.                                              \n",
      "Removing gs://dl_training_results/checkpoints//test...\n",
      "/ [1 objects]                                                                   \n",
      "Operation completed over 1 objects.                                              \n",
      "Testing dataset bucket connectivity\n",
      "gs://datasets_bucket_a/training-001.tfrecords\n",
      "gs://datasets_bucket_a/training-002.tfrecords\n",
      "gs://datasets_bucket_a/training-003.tfrecords\n",
      "gs://datasets_bucket_a/training-004.tfrecords\n",
      "Testing TPU connectivity\n",
      "\n",
      "Starting Nmap 7.40 ( https://nmap.org ) at 2019-12-10 02:32 UTC\n",
      "Nmap scan report for 10.0.3.2\n",
      "Host is up (0.0015s latency).\n",
      "PORT     STATE SERVICE\n",
      "8470/tcp open  cisco-avp\n",
      "\n",
      "Nmap done: 1 IP address (1 host up) scanned in 0.04 seconds\n",
      "Trying to connect to a TPU node\n",
      "\n",
      "!!!MAKE SURE THE TPU ADDRESS IS CORRECT!!\n",
      "1.ip must be correct\n",
      "2.tpu must be turned on\n",
      "3.version must be 'nightly-2.x'\n",
      "4.tpu must be reachable (check with gce netowrking/connectivity test)\n",
      "if not this will hang!\n",
      "\n",
      "Trying to connect to: grpc://10.0.3.2:8470\n",
      "INFO:tensorflow:Initializing the TPU system: grpc://10.0.3.2:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.0.3.2:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "if c.TPU_MODE:\n",
    "    print(\"Testing results bucket connectivity\")\n",
    "    !touch /tmp/test\n",
    "    !gsutil cp /tmp/test {c.TENSORBOARD_PATH}/test\n",
    "    !gsutil rm {c.TENSORBOARD_PATH}/test\n",
    "    !gsutil cp /tmp/test {c.CHECKPOINTS_PATH}/test\n",
    "    !gsutil rm {c.CHECKPOINTS_PATH}/test\n",
    "    print(\"Testing dataset bucket connectivity\")\n",
    "    !gsutil ls gs://{c.GCS_TFRECORDS_BUCKETNAME} | head -4\n",
    "    print(\"Testing TPU connectivity\")\n",
    "    !nmap -Pn -p8470 {c.TPU_IP}\n",
    "    import tpu_training.init_tpu as init_tpu\n",
    "    strategy,resolver=init_tpu.init_tpu() #This must be run before any imports!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import dataset_functions\n",
    "import models.six_stage_linear_model as model\n",
    "import callbacks\n",
    "import dataset_builder\n",
    "import load_weights\n",
    "import loss_metrics\n",
    "from utils import now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.six_stage_linear_model' from '/home/michael_zl_prime/project_drive/project/models/six_stage_linear_model.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib as il\n",
    "il.reload(model) #useful to reload any changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving TFrecords in TPU_mode\n",
      "Found the following TFrecords:\n",
      " gs://datasets_bucket_a/training-001.tfrecords\n",
      "gs://datasets_bucket_a/training-002.tfrecords\n",
      "gs://datasets_bucket_a/training-003.tfrecords\n",
      "gs://datasets_bucket_a/training-004.tfrecords\n",
      "gs://datasets_bucket_a/training-005.tfrecords\n",
      "gs://datasets_bucket_a/training-006.tfrecords\n",
      "gs://datasets_bucket_a/training-007.tfrecords\n",
      "gs://datasets_bucket_a/training-008.tfrecords\n",
      "gs://datasets_bucket_a/training-009.tfrecords\n",
      "gs://datasets_bucket_a/training-010.tfrecords\n",
      "gs://datasets_bucket_a/training-011.tfrecords\n",
      "gs://datasets_bucket_a/training-012.tfrecords\n",
      "gs://datasets_bucket_a/training-013.tfrecords\n",
      "gs://datasets_bucket_a/training-014.tfrecords\n",
      "gs://datasets_bucket_a/training-015.tfrecords\n",
      "gs://datasets_bucket_a/training-016.tfrecords\n",
      "gs://datasets_bucket_a/training-017.tfrecords\n",
      "gs://datasets_bucket_a/training-018.tfrecords\n",
      "gs://datasets_bucket_a/training-019.tfrecords\n",
      "gs://datasets_bucket_a/training-020.tfrecords\n",
      "gs://datasets_bucket_a/training-021.tfrecords\n",
      "gs://datasets_bucket_a/training-022.tfrecords\n",
      "gs://datasets_bucket_a/training-023.tfrecords\n",
      "gs://datasets_bucket_a/training-024.tfrecords\n",
      "gs://datasets_bucket_a/training-025.tfrecords\n",
      "gs://datasets_bucket_a/training-026.tfrecords\n",
      "gs://datasets_bucket_a/training-027.tfrecords\n",
      "gs://datasets_bucket_a/training-028.tfrecords\n",
      "gs://datasets_bucket_a/training-029.tfrecords\n",
      "gs://datasets_bucket_a/training-030.tfrecords\n",
      "gs://datasets_bucket_a/training-031.tfrecords\n",
      "gs://datasets_bucket_a/training-032.tfrecords\n",
      "gs://datasets_bucket_a/training-033.tfrecords\n",
      "gs://datasets_bucket_a/training-034.tfrecords\n",
      "gs://datasets_bucket_a/training-035.tfrecords\n",
      "gs://datasets_bucket_a/training-036.tfrecords\n",
      "gs://datasets_bucket_a/training-037.tfrecords\n",
      "gs://datasets_bucket_a/training-038.tfrecords\n",
      "gs://datasets_bucket_a/training-039.tfrecords\n",
      "gs://datasets_bucket_a/training-040.tfrecords\n",
      "gs://datasets_bucket_a/training-041.tfrecords\n",
      "gs://datasets_bucket_a/training-042.tfrecords\n",
      "gs://datasets_bucket_a/training-043.tfrecords\n",
      "gs://datasets_bucket_a/training-044.tfrecords\n",
      "gs://datasets_bucket_a/training-045.tfrecords\n",
      "gs://datasets_bucket_a/training-046.tfrecords\n",
      "gs://datasets_bucket_a/training-047.tfrecords\n",
      "gs://datasets_bucket_a/training-048.tfrecords\n",
      "gs://datasets_bucket_a/training-049.tfrecords\n",
      "gs://datasets_bucket_a/training-050.tfrecords\n",
      "gs://datasets_bucket_a/training-051.tfrecords\n",
      "gs://datasets_bucket_a/training-052.tfrecords\n",
      "gs://datasets_bucket_a/training-053.tfrecords\n",
      "gs://datasets_bucket_a/training-054.tfrecords\n",
      "gs://datasets_bucket_a/training-055.tfrecords\n",
      "gs://datasets_bucket_a/training-056.tfrecords\n",
      "gs://datasets_bucket_a/training-057.tfrecords \n",
      " gs://datasets_bucket_a/validation-001.tfrecords\n",
      "gs://datasets_bucket_a/validation-002.tfrecords\n",
      "gs://datasets_bucket_a/validation-003.tfrecords\n",
      "Building training dataset\n",
      "Training dataset shape: <PrefetchDataset shapes: (((None, 368, 368, 3), (None, 46, 46, 1)), ((None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 19), (None, 46, 46, 19))), types: ((tf.float32, tf.float32), (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32))>\n",
      "Building validation dataset\n",
      "Validation dataset shape: <BatchDataset shapes: (((None, 368, 368, 3), (None, 46, 46, 1)), ((None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 19), (None, 46, 46, 19))), types: ((tf.float32, tf.float32), (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32))>\n"
     ]
    }
   ],
   "source": [
    "tfrecord_files_train,tfrecord_files_val=dataset_builder.get_tfrecord_filenames()\n",
    "print(\"Found the following TFrecords:\\n\",\"\\n\".join(tfrecord_files_train),\"\\n\", \"\\n\".join(tfrecord_files_val))\n",
    "\n",
    "print(\"Building training dataset\")\n",
    "dst=dataset_builder.build_training_ds(tfrecord_files_train,model.place_training_labels)\n",
    "print(\"Training dataset shape:\",dst)\n",
    "print(\"Building validation dataset\")\n",
    "dsv=dataset_builder.build_validation_ds(tfrecord_files_val,model.place_training_labels)\n",
    "print(\"Validation dataset shape:\",dsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dst=dst.take(20)\n",
    "# dst=dst.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Test element\n",
    "# dst_iter=iter(dst)\n",
    "# sample_elem=next(dst_iter)\n",
    "# print(\"Dataset shape:\",dst) #this should match the model input, and output stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#testing mask\n",
    "# sample_elem=next(dst_iter)\n",
    "# m=sample_elem[1][1][0,...,0]\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(m)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fit_callbacks=[\n",
    "    callbacks.checkpoints_callback\n",
    "    ,callbacks.tensorboard_callback\n",
    "    ,callbacks.learning_rate_scheduler_callback\n",
    "    ,callbacks.print_lr_callback\n",
    "    ,tf.keras.callbacks.TerminateOnNaN()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "#run to clean tensorboard results\n",
    "#!gsutil -m rm -r {c.TENSORBOARD_PATH}/*\n",
    "#!gsutil -m rm -r {c.CHECKPOINTS_PATH}/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model\n",
    "## Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found these checkpoints\n",
      "0.Dont load checkpoint\n",
      "1 .gs://dl_training_results/checkpoints//I_think_I_figure_it_out10Tue1219-0159/Checkpoint-E0001.ckpt\n",
      "2 .gs://dl_training_results/checkpoints//I_think_I_figure_it_out10Tue1219-0159/Checkpoint-E0002.ckpt\n",
      "3 .gs://dl_training_results/checkpoints//I_think_I_figure_it_out10Tue1219-0159/Checkpoint-E0003.ckpt\n",
      "Please select checkpoint, or 0 to continue without loading0\n"
     ]
    }
   ],
   "source": [
    "if c.ASK_FOR_CHECKPOINTS:\n",
    "    checkpoint,starting_epoch=load_weights.checkpoints_prompt()\n",
    "else:\n",
    "    checkpoint=None\n",
    "    starting_epoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_maker=model.ModelMaker() #must be outside scope to keep the graph clean\n",
    "tf.keras.backend.clear_session() #to clean to backaend from the imported model\n",
    "def define():\n",
    "    train_model,test_model=model_maker.create_models()\n",
    "    \n",
    "#     if c.INCLUDE_MASK:\n",
    "#         losses=[loss_metrics.MaskedMeanAbsoluteError()\n",
    "#                 ,loss_metrics.MaskedMeanAbsoluteError()\n",
    "#                 ,loss_metrics.MaskedMeanAbsoluteError()\n",
    "#                 ,loss_metrics.MaskedMeanAbsoluteError()\n",
    "#                 ,loss_metrics.MaskedMeanSquaredError()\n",
    "#                 ,loss_metrics.MaskedMeanSquaredError()]\n",
    "                      \n",
    "#     else:\n",
    "#         raise NotImplementedError       \n",
    "    \n",
    "    #this must match the model output order\n",
    "#     metrics=[\n",
    "#           []\n",
    "#          ,[]\n",
    "#          ,[]\n",
    "#          ,[]\n",
    "#          ,[tf.keras.metrics.MeanAbsoluteError()]    \n",
    "#          ,[tf.keras.metrics.MeanAbsoluteError()]\n",
    "#         ]\n",
    "    \n",
    "    train_model.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(c.BASE_LEARNING_RATE)                   \n",
    "                    ,loss=loss_metrics.MaskedMeanSquaredError()\n",
    "                    ,loss_weights=[10,10,10,10,0.1,0.1]\n",
    "                    #,metrics=metrics                           \n",
    "                   )\n",
    "    return train_model,test_model\n",
    "\n",
    "if c.TPU_MODE:\n",
    "    with strategy.scope():\n",
    "        train_model,test_model=define()\n",
    "        if (checkpoint):\n",
    "            train_model.load_weights(checkpoint)\n",
    "else:\n",
    "    train_model,test_model=define()\n",
    "    if (checkpoint):\n",
    "        train_model.load_weights(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Actually training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 437 steps\n",
      "\n",
      "Learning rate for epoch 0 is 0.0010000000474974513\n",
      "Epoch 1/100\n",
      "  1/437 [..............................] - ETA: 8:53:12 - loss: 0.7824 - s1pafs_output_mask_loss: 0.0683 - s2pafs_output_mask_loss: 0.0354 - s3pafs_output_mask_loss: 0.0497 - s4pafs_output_mask_loss: 0.0551 - s5kpts_output_mask_loss: 0.2001 - s6kpts_output_mask_loss: 0.2107WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.091665). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.091665). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436/437 [============================>.] - ETA: 1s - loss: 0.1047 - s1pafs_output_mask_loss: 0.0050 - s2pafs_output_mask_loss: 0.0050 - s3pafs_output_mask_loss: 0.0062 - s4pafs_output_mask_loss: 0.0062 - s5kpts_output_mask_loss: 0.0409 - s6kpts_output_mask_loss: 0.0410\n",
      "Epoch 00001: saving model to gs://dl_training_results/checkpoints/I_think_I_figure_it_out_l210Tue1219-0232/Checkpoint-E0001.ckpt\n",
      "437/437 [==============================] - 542s 1s/step - loss: 0.1047 - s1pafs_output_mask_loss: 0.0050 - s2pafs_output_mask_loss: 0.0050 - s3pafs_output_mask_loss: 0.0062 - s4pafs_output_mask_loss: 0.0062 - s5kpts_output_mask_loss: 0.0409 - s6kpts_output_mask_loss: 0.0410 - val_loss: 0.0913 - val_s1pafs_output_mask_loss: 0.0048 - val_s2pafs_output_mask_loss: 0.0047 - val_s3pafs_output_mask_loss: 0.0046 - val_s4pafs_output_mask_loss: 0.0046 - val_s5kpts_output_mask_loss: 0.0400 - val_s6kpts_output_mask_loss: 0.0400\n",
      "\n",
      "Learning rate for epoch 1 is 0.0010000000474974513\n",
      "Epoch 2/100\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0912 - s1pafs_output_mask_loss: 0.0049 - s2pafs_output_mask_loss: 0.0046 - s3pafs_output_mask_loss: 0.0046 - s4pafs_output_mask_loss: 0.0045 - s5kpts_output_mask_loss: 0.0405 - s6kpts_output_mask_loss: 0.0405\n",
      "Epoch 00002: saving model to gs://dl_training_results/checkpoints/I_think_I_figure_it_out_l210Tue1219-0232/Checkpoint-E0002.ckpt\n",
      "437/437 [==============================] - 407s 932ms/step - loss: 0.0912 - s1pafs_output_mask_loss: 0.0049 - s2pafs_output_mask_loss: 0.0046 - s3pafs_output_mask_loss: 0.0046 - s4pafs_output_mask_loss: 0.0045 - s5kpts_output_mask_loss: 0.0405 - s6kpts_output_mask_loss: 0.0405 - val_loss: 0.0897 - val_s1pafs_output_mask_loss: 0.0048 - val_s2pafs_output_mask_loss: 0.0045 - val_s3pafs_output_mask_loss: 0.0045 - val_s4pafs_output_mask_loss: 0.0044 - val_s5kpts_output_mask_loss: 0.0400 - val_s6kpts_output_mask_loss: 0.0400\n",
      "\n",
      "Learning rate for epoch 2 is 0.0010000000474974513\n",
      "Epoch 3/100\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0918 - s1pafs_output_mask_loss: 0.0049 - s2pafs_output_mask_loss: 0.0046 - s3pafs_output_mask_loss: 0.0046 - s4pafs_output_mask_loss: 0.0046 - s5kpts_output_mask_loss: 0.0405 - s6kpts_output_mask_loss: 0.0405\n",
      "Epoch 00003: saving model to gs://dl_training_results/checkpoints/I_think_I_figure_it_out_l210Tue1219-0232/Checkpoint-E0003.ckpt\n",
      "437/437 [==============================] - 396s 906ms/step - loss: 0.0918 - s1pafs_output_mask_loss: 0.0049 - s2pafs_output_mask_loss: 0.0046 - s3pafs_output_mask_loss: 0.0046 - s4pafs_output_mask_loss: 0.0046 - s5kpts_output_mask_loss: 0.0405 - s6kpts_output_mask_loss: 0.0405 - val_loss: 0.0922 - val_s1pafs_output_mask_loss: 0.0048 - val_s2pafs_output_mask_loss: 0.0048 - val_s3pafs_output_mask_loss: 0.0047 - val_s4pafs_output_mask_loss: 0.0047 - val_s5kpts_output_mask_loss: 0.0400 - val_s6kpts_output_mask_loss: 0.0400\n",
      "\n",
      "Learning rate for epoch 3 is 0.0010000000474974513\n",
      "Epoch 4/100\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0905 - s1pafs_output_mask_loss: 0.0047 - s2pafs_output_mask_loss: 0.0046 - s3pafs_output_mask_loss: 0.0045 - s4pafs_output_mask_loss: 0.0044 - s5kpts_output_mask_loss: 0.0406 - s6kpts_output_mask_loss: 0.0406\n",
      "Epoch 00004: saving model to gs://dl_training_results/checkpoints/I_think_I_figure_it_out_l210Tue1219-0232/Checkpoint-E0004.ckpt\n",
      "437/437 [==============================] - 401s 918ms/step - loss: 0.0905 - s1pafs_output_mask_loss: 0.0047 - s2pafs_output_mask_loss: 0.0046 - s3pafs_output_mask_loss: 0.0045 - s4pafs_output_mask_loss: 0.0044 - s5kpts_output_mask_loss: 0.0406 - s6kpts_output_mask_loss: 0.0406 - val_loss: 0.0877 - val_s1pafs_output_mask_loss: 0.0045 - val_s2pafs_output_mask_loss: 0.0043 - val_s3pafs_output_mask_loss: 0.0043 - val_s4pafs_output_mask_loss: 0.0043 - val_s5kpts_output_mask_loss: 0.0400 - val_s6kpts_output_mask_loss: 0.0400\n",
      "\n",
      "Learning rate for epoch 4 is 0.0010000000474974513\n",
      "Epoch 5/100\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0858 - s1pafs_output_mask_loss: 0.0044 - s2pafs_output_mask_loss: 0.0041 - s3pafs_output_mask_loss: 0.0041 - s4pafs_output_mask_loss: 0.0040 - s5kpts_output_mask_loss: 0.0406 - s6kpts_output_mask_loss: 0.0406\n",
      "Epoch 00005: saving model to gs://dl_training_results/checkpoints/I_think_I_figure_it_out_l210Tue1219-0232/Checkpoint-E0005.ckpt\n",
      "437/437 [==============================] - 397s 909ms/step - loss: 0.0857 - s1pafs_output_mask_loss: 0.0044 - s2pafs_output_mask_loss: 0.0041 - s3pafs_output_mask_loss: 0.0041 - s4pafs_output_mask_loss: 0.0040 - s5kpts_output_mask_loss: 0.0405 - s6kpts_output_mask_loss: 0.0405 - val_loss: 0.0818 - val_s1pafs_output_mask_loss: 0.0040 - val_s2pafs_output_mask_loss: 0.0038 - val_s3pafs_output_mask_loss: 0.0038 - val_s4pafs_output_mask_loss: 0.0037 - val_s5kpts_output_mask_loss: 0.0400 - val_s6kpts_output_mask_loss: 0.0400\n",
      "\n",
      "Learning rate for epoch 5 is 0.0010000000474974513\n",
      "Epoch 6/100\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0799 - s1pafs_output_mask_loss: 0.0038 - s2pafs_output_mask_loss: 0.0036 - s3pafs_output_mask_loss: 0.0035 - s4pafs_output_mask_loss: 0.0035 - s5kpts_output_mask_loss: 0.0405 - s6kpts_output_mask_loss: 0.0405\n",
      "Epoch 00006: saving model to gs://dl_training_results/checkpoints/I_think_I_figure_it_out_l210Tue1219-0232/Checkpoint-E0006.ckpt\n",
      "437/437 [==============================] - 400s 916ms/step - loss: 0.0799 - s1pafs_output_mask_loss: 0.0038 - s2pafs_output_mask_loss: 0.0036 - s3pafs_output_mask_loss: 0.0035 - s4pafs_output_mask_loss: 0.0035 - s5kpts_output_mask_loss: 0.0405 - s6kpts_output_mask_loss: 0.0405 - val_loss: 0.0780 - val_s1pafs_output_mask_loss: 0.0037 - val_s2pafs_output_mask_loss: 0.0036 - val_s3pafs_output_mask_loss: 0.0034 - val_s4pafs_output_mask_loss: 0.0033 - val_s5kpts_output_mask_loss: 0.0400 - val_s6kpts_output_mask_loss: 0.0400\n",
      "\n",
      "Learning rate for epoch 6 is 0.0010000000474974513\n",
      "Epoch 7/100\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0766 - s1pafs_output_mask_loss: 0.0035 - s2pafs_output_mask_loss: 0.0034 - s3pafs_output_mask_loss: 0.0032 - s4pafs_output_mask_loss: 0.0032 - s5kpts_output_mask_loss: 0.0405 - s6kpts_output_mask_loss: 0.0405\n",
      "Epoch 00007: saving model to gs://dl_training_results/checkpoints/I_think_I_figure_it_out_l210Tue1219-0232/Checkpoint-E0007.ckpt\n",
      "437/437 [==============================] - 397s 907ms/step - loss: 0.0766 - s1pafs_output_mask_loss: 0.0035 - s2pafs_output_mask_loss: 0.0034 - s3pafs_output_mask_loss: 0.0032 - s4pafs_output_mask_loss: 0.0032 - s5kpts_output_mask_loss: 0.0405 - s6kpts_output_mask_loss: 0.0405 - val_loss: 0.0755 - val_s1pafs_output_mask_loss: 0.0034 - val_s2pafs_output_mask_loss: 0.0033 - val_s3pafs_output_mask_loss: 0.0032 - val_s4pafs_output_mask_loss: 0.0031 - val_s5kpts_output_mask_loss: 0.0400 - val_s6kpts_output_mask_loss: 0.0400\n",
      "\n",
      "Learning rate for epoch 7 is 0.0010000000474974513\n",
      "Epoch 8/100\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0743 - s1pafs_output_mask_loss: 0.0033 - s2pafs_output_mask_loss: 0.0031 - s3pafs_output_mask_loss: 0.0030 - s4pafs_output_mask_loss: 0.0030 - s5kpts_output_mask_loss: 0.0406 - s6kpts_output_mask_loss: 0.0406\n",
      "Epoch 00008: saving model to gs://dl_training_results/checkpoints/I_think_I_figure_it_out_l210Tue1219-0232/Checkpoint-E0008.ckpt\n",
      "437/437 [==============================] - 395s 903ms/step - loss: 0.0743 - s1pafs_output_mask_loss: 0.0033 - s2pafs_output_mask_loss: 0.0031 - s3pafs_output_mask_loss: 0.0030 - s4pafs_output_mask_loss: 0.0030 - s5kpts_output_mask_loss: 0.0406 - s6kpts_output_mask_loss: 0.0406 - val_loss: 0.0735 - val_s1pafs_output_mask_loss: 0.0032 - val_s2pafs_output_mask_loss: 0.0031 - val_s3pafs_output_mask_loss: 0.0030 - val_s4pafs_output_mask_loss: 0.0029 - val_s5kpts_output_mask_loss: 0.0400 - val_s6kpts_output_mask_loss: 0.0400\n",
      "\n",
      "Learning rate for epoch 8 is 0.0010000000474974513\n",
      "Epoch 9/100\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.559481). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.559481). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436/437 [============================>.] - ETA: 0s - loss: 0.0724 - s1pafs_output_mask_loss: 0.0031 - s2pafs_output_mask_loss: 0.0030 - s3pafs_output_mask_loss: 0.0029 - s4pafs_output_mask_loss: 0.0028 - s5kpts_output_mask_loss: 0.0405 - s6kpts_output_mask_loss: 0.0405\n",
      "Epoch 00009: saving model to gs://dl_training_results/checkpoints/I_think_I_figure_it_out_l210Tue1219-0232/Checkpoint-E0009.ckpt\n",
      "437/437 [==============================] - 396s 906ms/step - loss: 0.0724 - s1pafs_output_mask_loss: 0.0031 - s2pafs_output_mask_loss: 0.0030 - s3pafs_output_mask_loss: 0.0029 - s4pafs_output_mask_loss: 0.0028 - s5kpts_output_mask_loss: 0.0405 - s6kpts_output_mask_loss: 0.0405 - val_loss: 0.0722 - val_s1pafs_output_mask_loss: 0.0031 - val_s2pafs_output_mask_loss: 0.0030 - val_s3pafs_output_mask_loss: 0.0029 - val_s4pafs_output_mask_loss: 0.0028 - val_s5kpts_output_mask_loss: 0.0400 - val_s6kpts_output_mask_loss: 0.0400\n",
      "\n",
      "Learning rate for epoch 9 is 0.0010000000474974513\n",
      "Epoch 10/100\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0712 - s1pafs_output_mask_loss: 0.0030 - s2pafs_output_mask_loss: 0.0029 - s3pafs_output_mask_loss: 0.0027 - s4pafs_output_mask_loss: 0.0027 - s5kpts_output_mask_loss: 0.0405 - s6kpts_output_mask_loss: 0.0405\n",
      "Epoch 00010: saving model to gs://dl_training_results/checkpoints/I_think_I_figure_it_out_l210Tue1219-0232/Checkpoint-E0010.ckpt\n",
      "437/437 [==============================] - 400s 914ms/step - loss: 0.0712 - s1pafs_output_mask_loss: 0.0030 - s2pafs_output_mask_loss: 0.0029 - s3pafs_output_mask_loss: 0.0027 - s4pafs_output_mask_loss: 0.0027 - s5kpts_output_mask_loss: 0.0405 - s6kpts_output_mask_loss: 0.0405 - val_loss: 0.0711 - val_s1pafs_output_mask_loss: 0.0031 - val_s2pafs_output_mask_loss: 0.0029 - val_s3pafs_output_mask_loss: 0.0028 - val_s4pafs_output_mask_loss: 0.0027 - val_s5kpts_output_mask_loss: 0.0400 - val_s6kpts_output_mask_loss: 0.0400\n",
      "\n",
      "Learning rate for epoch 10 is 0.0010000000474974513\n",
      "Epoch 11/100\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0700 - s1pafs_output_mask_loss: 0.0029 - s2pafs_output_mask_loss: 0.0028 - s3pafs_output_mask_loss: 0.0026 - s4pafs_output_mask_loss: 0.0025 - s5kpts_output_mask_loss: 0.0405 - s6kpts_output_mask_loss: 0.0405\n",
      "Epoch 00011: saving model to gs://dl_training_results/checkpoints/I_think_I_figure_it_out_l210Tue1219-0232/Checkpoint-E0011.ckpt\n",
      "437/437 [==============================] - 402s 921ms/step - loss: 0.0700 - s1pafs_output_mask_loss: 0.0029 - s2pafs_output_mask_loss: 0.0028 - s3pafs_output_mask_loss: 0.0026 - s4pafs_output_mask_loss: 0.0025 - s5kpts_output_mask_loss: 0.0405 - s6kpts_output_mask_loss: 0.0405 - val_loss: 0.0706 - val_s1pafs_output_mask_loss: 0.0030 - val_s2pafs_output_mask_loss: 0.0029 - val_s3pafs_output_mask_loss: 0.0027 - val_s4pafs_output_mask_loss: 0.0027 - val_s5kpts_output_mask_loss: 0.0400 - val_s6kpts_output_mask_loss: 0.0400\n",
      "\n",
      "Learning rate for epoch 11 is 0.0010000000474974513\n",
      "Epoch 12/100\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0690 - s1pafs_output_mask_loss: 0.0029 - s2pafs_output_mask_loss: 0.0027 - s3pafs_output_mask_loss: 0.0025 - s4pafs_output_mask_loss: 0.0024 - s5kpts_output_mask_loss: 0.0405 - s6kpts_output_mask_loss: 0.0405\n",
      "Epoch 00012: saving model to gs://dl_training_results/checkpoints/I_think_I_figure_it_out_l210Tue1219-0232/Checkpoint-E0012.ckpt\n",
      "437/437 [==============================] - 397s 910ms/step - loss: 0.0690 - s1pafs_output_mask_loss: 0.0029 - s2pafs_output_mask_loss: 0.0027 - s3pafs_output_mask_loss: 0.0025 - s4pafs_output_mask_loss: 0.0024 - s5kpts_output_mask_loss: 0.0405 - s6kpts_output_mask_loss: 0.0405 - val_loss: 0.0700 - val_s1pafs_output_mask_loss: 0.0030 - val_s2pafs_output_mask_loss: 0.0028 - val_s3pafs_output_mask_loss: 0.0027 - val_s4pafs_output_mask_loss: 0.0026 - val_s5kpts_output_mask_loss: 0.0400 - val_s6kpts_output_mask_loss: 0.0400\n",
      "\n",
      "Learning rate for epoch 12 is 0.0010000000474974513\n",
      "Epoch 13/100\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0683 - s1pafs_output_mask_loss: 0.0028 - s2pafs_output_mask_loss: 0.0026 - s3pafs_output_mask_loss: 0.0025 - s4pafs_output_mask_loss: 0.0024 - s5kpts_output_mask_loss: 0.0406 - s6kpts_output_mask_loss: 0.0406\n",
      "Epoch 00013: saving model to gs://dl_training_results/checkpoints/I_think_I_figure_it_out_l210Tue1219-0232/Checkpoint-E0013.ckpt\n",
      "437/437 [==============================] - 396s 905ms/step - loss: 0.0683 - s1pafs_output_mask_loss: 0.0028 - s2pafs_output_mask_loss: 0.0026 - s3pafs_output_mask_loss: 0.0025 - s4pafs_output_mask_loss: 0.0024 - s5kpts_output_mask_loss: 0.0406 - s6kpts_output_mask_loss: 0.0406 - val_loss: 0.0697 - val_s1pafs_output_mask_loss: 0.0030 - val_s2pafs_output_mask_loss: 0.0028 - val_s3pafs_output_mask_loss: 0.0026 - val_s4pafs_output_mask_loss: 0.0026 - val_s5kpts_output_mask_loss: 0.0400 - val_s6kpts_output_mask_loss: 0.0400\n",
      "\n",
      "Learning rate for epoch 13 is 0.0010000000474974513\n",
      "Epoch 14/100\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0676 - s1pafs_output_mask_loss: 0.0028 - s2pafs_output_mask_loss: 0.0026 - s3pafs_output_mask_loss: 0.0024 - s4pafs_output_mask_loss: 0.0023 - s5kpts_output_mask_loss: 0.0405 - s6kpts_output_mask_loss: 0.0405\n",
      "Epoch 00014: saving model to gs://dl_training_results/checkpoints/I_think_I_figure_it_out_l210Tue1219-0232/Checkpoint-E0014.ckpt\n",
      "437/437 [==============================] - 407s 932ms/step - loss: 0.0676 - s1pafs_output_mask_loss: 0.0028 - s2pafs_output_mask_loss: 0.0026 - s3pafs_output_mask_loss: 0.0024 - s4pafs_output_mask_loss: 0.0023 - s5kpts_output_mask_loss: 0.0405 - s6kpts_output_mask_loss: 0.0405 - val_loss: 0.0690 - val_s1pafs_output_mask_loss: 0.0029 - val_s2pafs_output_mask_loss: 0.0027 - val_s3pafs_output_mask_loss: 0.0026 - val_s4pafs_output_mask_loss: 0.0025 - val_s5kpts_output_mask_loss: 0.0400 - val_s6kpts_output_mask_loss: 0.0400\n",
      "\n",
      "Learning rate for epoch 14 is 0.0010000000474974513\n",
      "Epoch 15/100\n",
      " 22/437 [>.............................] - ETA: 4:33 - loss: 0.0660 - s1pafs_output_mask_loss: 0.0027 - s2pafs_output_mask_loss: 0.0025 - s3pafs_output_mask_loss: 0.0023 - s4pafs_output_mask_loss: 0.0022 - s5kpts_output_mask_loss: 0.0400 - s6kpts_output_mask_loss: 0.0400\n",
      "Epoch 00015: saving model to gs://dl_training_results/checkpoints/I_think_I_figure_it_out_l210Tue1219-0232/Checkpoint-E0015.ckpt\n",
      " 22/437 [>.............................] - ETA: 11:09 - loss: 0.0660 - s1pafs_output_mask_loss: 0.0027 - s2pafs_output_mask_loss: 0.0025 - s3pafs_output_mask_loss: 0.0023 - s4pafs_output_mask_loss: 0.0022 - s5kpts_output_mask_loss: 0.0400 - s6kpts_output_mask_loss: 0.0400"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c6990a125fe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m,\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstarting_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m   \u001b[0mconstant_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[0;34m(tensor, partial)\u001b[0m\n\u001b[1;32m    820\u001b[0m   \"\"\"\n\u001b[1;32m    821\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \"\"\"\n\u001b[1;32m    941\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_history=train_model.fit(\n",
    "    dst\n",
    "    ,epochs=c.TRAINING_EPOCHS\n",
    "    ,steps_per_epoch=c.STEPS_PER_EPOCH\n",
    "    ,validation_data=dsv\n",
    "    ,callbacks=fit_callbacks\n",
    "    ,initial_epoch=starting_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1788: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1788: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://dl_training_results/models/test_I_think_I_figure_it_out_l210Tue1219-0409I_think_I_figure_it_out_l2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://dl_training_results/models/test_I_think_I_figure_it_out_l210Tue1219-0409I_think_I_figure_it_out_l2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://dl_training_results/models/train_I_think_I_figure_it_out_l210Tue1219-0409I_think_I_figure_it_out_l2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://dl_training_results/models/train_I_think_I_figure_it_out_l210Tue1219-0409I_think_I_figure_it_out_l2/assets\n"
     ]
    }
   ],
   "source": [
    "tmp_path='gs://dl_training_results/tmp/tensorflow/temp_weights'\n",
    "train_model.save_weights(tmp_path)\n",
    "\n",
    "cpu_train_model,cpu_test_model=define()\n",
    "\n",
    "cpu_train_model.load_weights(tmp_path)\n",
    "cpu_test_model.load_weights(tmp_path)\n",
    "\n",
    "cpu_test_model.save(c.MODELS_PATH+\"test_\"+now()+c.RUN_NAME) \n",
    "cpu_train_model.save(c.MODELS_PATH+\"train_\"+now()+c.RUN_NAME) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request issued for: [node-1]\n",
      "Waiting for operation [projects/deeplearning-257902/locations/us-central1-c/ope\n",
      "rations/operation-1575950998856-59951af786436-35014774-6248d9b5] to complete...\n",
      "done.                                                                          \n",
      "done: true\n",
      "metadata:\n",
      "  '@type': type.googleapis.com/google.cloud.common.OperationMetadata\n",
      "  apiVersion: v1\n",
      "  cancelRequested: false\n",
      "  createTime: '2019-12-10T04:09:58.872940998Z'\n",
      "  endTime: '2019-12-10T04:11:32.290331677Z'\n",
      "  target: projects/deeplearning-257902/locations/us-central1-c/nodes/node-1\n",
      "  verb: update\n",
      "name: projects/deeplearning-257902/locations/us-central1-c/operations/operation-1575950998856-59951af786436-35014774-6248d9b5\n",
      "response:\n",
      "  '@type': type.googleapis.com/google.cloud.tpu.v1.Node\n",
      "  acceleratorType: v2-8\n",
      "  cidrBlock: 10.0.3.0/29\n",
      "  createTime: '2019-11-26T18:14:01.401101656Z'\n",
      "  health: HEALTHY\n",
      "  ipAddress: 10.0.3.2\n",
      "  name: projects/deeplearning-257902/locations/us-central1-c/nodes/node-1\n",
      "  network: global/networks/default\n",
      "  networkEndpoints:\n",
      "  - ipAddress: 10.0.3.2\n",
      "    port: 8470\n",
      "  port: '8470'\n",
      "  schedulingConfig:\n",
      "    preemptible: true\n",
      "  serviceAccount: service-312164605303@cloud-tpu.iam.gserviceaccount.com\n",
      "  state: STOPPED\n",
      "  tensorflowVersion: nightly-2.x\n"
     ]
    }
   ],
   "source": [
    "#shut down TPU\n",
    "!gcloud compute tpus stop node-1 --zone us-central1-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud compute instances stop instance-1 --zone us-central1-c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
