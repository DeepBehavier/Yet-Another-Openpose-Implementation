{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#TODO\n",
    "Dataset side\n",
    "* add apropirate settings for TFRecordDataset -V\n",
    "* cache (before transformations) just to avoid disk access (should be ~9gigs) -V\n",
    "* move cache to ramfs -V\n",
    "* add augmentation ,after cache, probably before transformations \n",
    "* add prefetch, shuffle -V \n",
    "* create validation dataset -V\n",
    "\n",
    "Compilation side\n",
    "* callbacks:\n",
    "    *tensorboard -V\n",
    "    *saving weights every epoch -V\n",
    "    *learning rate -V\n",
    "    *\n",
    "* add tensorboard callback -V\n",
    "\n",
    "* add hyper parameters (learning rate, learning rate decay)\n",
    "* Figure out metrics (accuracy)\n",
    "    *add recall ,mean absoulte error, and 'island' recall error -V\n",
    "    *sort metrics by stage -V\n",
    "* add validation -V\n",
    "\n",
    "All\n",
    "* add comments\n",
    "\n",
    "TPUs\n",
    "* try with TPUs -VVV\n",
    "\n",
    "*Project\n",
    "add augmentation\n",
    "add regularization (dropout)\n",
    "\n",
    "Dangling layers\n",
    "#solve that issue -V\n",
    "\n",
    "Results\n",
    "*caching seems to eat only gpu memory\n",
    "*Intersting to try a ramfs or a BytesIO object , (ramfs is probably better though as tf handles the interface)\n",
    "*This should allow to increase batch size\n",
    "-Full epoc currently is 1:15h\n",
    "TPUs\n",
    "-After figuring out TPUs, 13min epoch is achieved!\n",
    "\n",
    "-execution adjustments, batch siez, caching.\n",
    "-\n",
    "\n",
    "Need to figure out what's going on with the PAF stages, all losses seem the same.\n",
    "1.maybe the same loss function is applied to all.\n",
    "check names of stages, and maybe have different losses for each.\n",
    "check model outputs and graphs\n",
    "2.maybe the dataset is all zeros for pafs, or the limbs are too small,\n",
    "optionally disable feedforward links\n",
    "3.another option is that it's ok\n",
    "\n",
    "observations\n",
    "*when the gradients explode, the PAF stages losses are very different:\n",
    "loss: 6034892169562175.0000 - stage1paf_output_loss: 2.7209 - stage2paf_output_loss: 3066.9797 - stage3paf_output_loss: 1367401.7500 - stage4paf_output_loss: 648865408.0000\n",
    "\n",
    "explosing gradients seem to have something with reloading checkpoints.\n",
    "if true I should open an issue, on econd look it might just be the learning rate.\n",
    "\n",
    "Learning rate\n",
    "0.0001 works fine \n",
    "0.003 explodes\n",
    "0.002 is too much as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "desktop-2qv6e1h\\flash\n",
      "      Credentialed Accounts\nACTIVE  ACCOUNT\n*       michael.zl.prime@gmail.com\nPython running from: c:\\program files\\python36-64\nCurrent working dir C:\\Users\\flash\\Project\\Yet-Another-Openpose-Implmentation\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "\nTo set the active account, run:\n    $ gcloud config set account `ACCOUNT`\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#verify user\n",
    "!whoami\n",
    "#verify user account, if running on google cloud, otherwise ignore\n",
    "!gcloud auth list\n",
    "#which enciorment/virtualenv running in\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"Python running from:\",sys.prefix)\n",
    "print(\"Current working dir\",os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#start tensor board\n",
    "# must run \n",
    "#/usr/local/bin/tensorboard serve --logdir gs://dl_training_results/tensorboard --port 8889 --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Tensorflow version: 2.0.0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "import dataset_functions\n",
    "from models.six_stage_linear_model import ModelMaker\n",
    "import callbacks\n",
    "import dataset_builder\n",
    "from config import *\n",
    "import load_weights\n",
    "import loss_metrics\n",
    "\n",
    "print(\"Tensorflow version:\",tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#override config\n",
    "TPU_MODE=False\n",
    "\n",
    "BATCH_SIZE=2\n",
    "BASE_LEARNING_RATE=0.001\n",
    "LEARNING_RATE_SCHEDUELE=np.zeros(1000)\n",
    "LEARNING_RATE_SCHEDUELE[:3]=0.2\n",
    "LEARNING_RATE_SCHEDUELE[3:20]=1\n",
    "LEARNING_RATE_SCHEDUELE[20:40]=1\n",
    "LEARNING_RATE_SCHEDUELE[40:100]=1\n",
    "LEARNING_RATE_SCHEDUELE[100:]=0.3\n",
    "LEARNING_RATE_SCHEDUELE*=BASE_LEARNING_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "if TPU_MODE:\n",
    "    #import tpu_training.test_connectivity\n",
    "    #tpu_training.test_connectivity.test_connectivity()\n",
    "    print(\"Testing results bucket connectivity\")\n",
    "    !touch /tmp/test\n",
    "    !gsutil cp /tmp/test {TENSORBOARD_PATH}/test\n",
    "    !gsutil rm {TENSORBOARD_PATH}/test\n",
    "    !gsutil cp /tmp/test {CHECKPOINTS_PATH}/test\n",
    "    !gsutil rm {CHECKPOINTS_PATH}/test\n",
    "    print(\"Testing dataset bucket connectivity\")\n",
    "    !gsutil ls gs://{GCS_TFRECORDS_BUCKETNAME} | head -4\n",
    "    print(\"Testing TPU connectivity\")\n",
    "    !nmap -Pn -p8470 {TPU_IP}\n",
    "    import tpu_training.init_tpu as init_tpu\n",
    "    strategy,resolver=init_tpu.init_tpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#tf.config.experimental_connect_to_cluster(resolver)\n",
    "#tpu_topo=tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "#tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Retrieving TFrecords in local mode\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "TF_parser = dataset_functions.TFrecordParser()\n",
    "tfrecord_files_train,tfrecord_files_val=dataset_builder.get_tfrecord_filenames()\n",
    "def build_training_ds(tfrecord_filenames:list)->tf.data.Dataset:\n",
    "    \"\"\"Generate training dataset from TFrecord file locations\n",
    "    :param tfrecord_files should be list of correct TFrecord filename, either local or remote (gcs, with gs:// prefix)\"\"\"\n",
    "    # TFrecord files to raw format\n",
    "    ds = tf.data.TFRecordDataset(tfrecord_filenames)  # numf reads can be put here, but I don't think I/O is the bottleneck\n",
    "\n",
    "    # raw format to imgs,tensors(coords kpts)\n",
    "    ds = ds.map(TF_parser.read_tfrecord)\n",
    "\n",
    "    # cache  ,caching is here before decompressing jpgs and label tensors (should be ~9GB) , (full dataset should be ~90, cache later if RAM aviable)\n",
    "    if CACHE: ds = ds.cache()\n",
    "    if SHUFFLE: ds = ds.shuffle(100)\n",
    "\n",
    "    # Augmentation should be here, to operate on smaller tensors\n",
    "\n",
    "    # imgs,tensors to label_tensors (46,46,17/38)\n",
    "    ds = ds.map(dataset_functions.make_label_tensors)\n",
    "    # imgs,label_tensors arrange for model outputs\n",
    "    ds = ds.map(dataset_functions.place_training_labels)\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.repeat()\n",
    "    if PREFETCH: ds = ds.prefetch(PREFETCH)\n",
    "    return ds\n",
    "dst=build_training_ds(tfrecord_files_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Retrieving TFrecords in local mode\nFound the following TFrecords:\n .\\dataset\\TFrecords\\training-001.tfrecords\n.\\dataset\\TFrecords\\training-002.tfrecords \n .\\dataset\\TFrecords\\validation-001.tfrecords\nBuilding training dataset\nTraining dataset shape: <PrefetchDataset shapes: (((None, 46, 46, 1), (None, 368, 368, 3)), ((None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 19), (None, 46, 46, 19))), types: ((tf.float32, tf.float32), (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32))>\nBuilding validation dataset\n",
      "Validation dataset shape: <BatchDataset shapes: (((None, 46, 46, 1), (None, 368, 368, 3)), ((None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 19), (None, 46, 46, 19))), types: ((tf.float32, tf.float32), (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32))>\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "tfrecord_files_train,tfrecord_files_val=dataset_builder.get_tfrecord_filenames()\n",
    "print(\"Found the following TFrecords:\\n\",\"\\n\".join(tfrecord_files_train),\"\\n\", \"\\n\".join(tfrecord_files_val))\n",
    "\n",
    "print(\"Building training dataset\")\n",
    "dst=dataset_builder.build_training_ds(tfrecord_files_train)\n",
    "print(\"Training dataset shape:\",dst)\n",
    "print(\"Building validation dataset\")\n",
    "dsv=dataset_builder.build_validation_ds(tfrecord_files_val)\n",
    "print(\"Validation dataset shape:\",dsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Test element\n",
    "# dst_iter=iter(dst)\n",
    "# sample_elem=next(dst_iter)\n",
    "# print(\"Dataset shape:\",dst) #this should match the model input, and output stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#testing mask\n",
    "# sample_elem=next(dst_iter)\n",
    "# m=sample_elem[1][1][0,...,0]\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(m)\n",
    "# plt.colorbar()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fit_callbacks=[\n",
    "    callbacks.checkpoints_callback\n",
    "    ,callbacks.tensorboard_callback\n",
    "    ,callbacks.learning_rate_scheduler_callback\n",
    "    ,callbacks.print_lr_callback\n",
    "    ,tf.keras.callbacks.TerminateOnNaN()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "#run to clean tensorboard results\n",
    "#!gsutil -m rm -r {TENSORBOARD_PATH}/*\n",
    "#!gsutil -m rm -r {CHECKPOINTS_PATH}/*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model\n",
    "## Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "checkpoint=None\n",
    "starting_epoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#checkpoint,starting_epoch=load_weights.checkpoints_prompt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if INCLUDE_MASK:\n",
    "    loss=loss_metrics.MaskedMeanSquaredError\n",
    "    loss_name=\"Masked_MSE\"\n",
    "else:\n",
    "    loss=tf.keras.losses.MeanSquaredError\n",
    "    loss_name=\"Regular_MSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "model_maker=ModelMaker() #must be outside scope to keep the graph clean\n",
    "tf.keras.backend.clear_session() #to clean to backaend from the imported model\n",
    "def define():\n",
    "    train_model,test_model=model_maker.create_models()\n",
    "    \n",
    "    #this must match the model output order\n",
    "    metrics=[\n",
    "          [tf.keras.metrics.MeanAbsoluteError()]\n",
    "         ,[tf.keras.metrics.MeanAbsoluteError()]\n",
    "         ,[tf.keras.metrics.MeanAbsoluteError()]\n",
    "         ,[tf.keras.metrics.MeanAbsoluteError()]\n",
    "         ,[#tf.keras.metrics.Recall(),\n",
    "                    tf.keras.metrics.MeanAbsoluteError()]    \n",
    "         ,[#tf.keras.metrics.Recall(),\n",
    "                    tf.keras.metrics.MeanAbsoluteError()]\n",
    "        ]\n",
    "    \n",
    "    train_model.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(BASE_LEARNING_RATE)\n",
    "                    ,loss=loss(name=loss_name)\n",
    "                    ,metrics=metrics\n",
    "                    #,sample_weight_mode=\"temporal\"        \n",
    "                   )\n",
    "    return train_model,test_model\n",
    "\n",
    "if TPU_MODE:\n",
    "    with strategy.scope():\n",
    "        train_model,test_model=define()\n",
    "        if (checkpoint):\n",
    "            train_model.load_weights(checkpoint)\n",
    "else:\n",
    "    train_model,test_model=define()\n",
    "    if (checkpoint):\n",
    "        train_model.load_weights(checkpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Actually training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train for 28000 steps\n",
      "\nLearning rate for epoch 0 is 0.00019999999494757503\nEpoch 1/100\n",
      "\r    1/28000 [..............................] - ETA: 289:24:58 - loss: 0.2339 - stage1paf_output_mask_loss: 0.0422 - stage2paf_output_mask_loss: 0.0318 - stage3paf_output_mask_loss: 0.0373 - stage4paf_output_mask_loss: 0.0281 - stage5heatmap_output_mask_loss: 0.0448 - stage6heatmap_output_mask_loss: 0.0496 - stage1paf_output_mask_mean_absolute_error: 0.1176 - stage2paf_output_mask_mean_absolute_error: 0.0988 - stage3paf_output_mask_mean_absolute_error: 0.0975 - stage4paf_output_mask_mean_absolute_error: 0.0872 - stage5heatmap_output_mask_mean_absolute_error: 0.1106 - stage6heatmap_output_mask_mean_absolute_error: 0.1332",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    2/28000 [..............................] - ETA: 223:14:21 - loss: 0.1543 - stage1paf_output_mask_loss: 0.0238 - stage2paf_output_mask_loss: 0.0184 - stage3paf_output_mask_loss: 0.0213 - stage4paf_output_mask_loss: 0.0173 - stage5heatmap_output_mask_loss: 0.0356 - stage6heatmap_output_mask_loss: 0.0380 - stage1paf_output_mask_mean_absolute_error: 0.0724 - stage2paf_output_mask_mean_absolute_error: 0.0580 - stage3paf_output_mask_mean_absolute_error: 0.0602 - stage4paf_output_mask_mean_absolute_error: 0.0587 - stage5heatmap_output_mask_mean_absolute_error: 0.0863 - stage6heatmap_output_mask_mean_absolute_error: 0.0991"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "train_history=train_model.fit(\n",
    "    dst\n",
    "    ,epochs=TRAINING_EPOCHS\n",
    "    ,steps_per_epoch=STEPS_PER_EPOCH\n",
    "    ,validation_data=dsv\n",
    "    ,callbacks=fit_callbacks\n",
    "    ,initial_epoch=starting_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model_path=MODELS_PATH+datetime.datetime.now().strftime(\"%d(%a)%m%y-%H%M\")\n",
    "tf.keras.models.save_model(train_model,model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#v.show_pafs_kpts_img(img.numpy(),paf.numpy(),kpt.numpy(),1,1) #can be used to draw the tensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}