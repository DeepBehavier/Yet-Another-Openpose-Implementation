{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO\n",
    "# Dataset side\n",
    "# * add apropirate settings for TFRecordDataset -V\n",
    "# * cache (before transformations) just to avoid disk access (should be ~9gigs) -V\n",
    "# * move cache to ramfs -V\n",
    "# * add augmentation ,after cache, probably before transformations \n",
    "# * add prefetch, shuffle -V \n",
    "# * create validation dataset -V\n",
    "\n",
    "# Compilation side\n",
    "# * callbacks:\n",
    "#     *tensorboard -V\n",
    "#     *saving weights every epoch -V\n",
    "#     *learning rate -V\n",
    "#     *\n",
    "# * add tensorboard callback -V\n",
    "\n",
    "# * add hyper parameters (learning rate, learning rate decay)\n",
    "# * Figure out metrics (accuracy)\n",
    "#     *add recall ,mean absoulte error, and 'island' recall error -V\n",
    "#     *sort metrics by stage -V\n",
    "# * add validation -V\n",
    "\n",
    "# All\n",
    "# * add comments\n",
    "\n",
    "# TPUs\n",
    "# * try with TPUs -VVV\n",
    "\n",
    "# *Project\n",
    "# add augmentation\n",
    "# add regularization (dropout)\n",
    "\n",
    "# Dangling layers\n",
    "# #solve that issue -V\n",
    "\n",
    "# Results\n",
    "# *caching seems to eat only gpu memory\n",
    "# *Intersting to try a ramfs or a BytesIO object , (ramfs is probably better though as tf handles the interface)\n",
    "# *This should allow to increase batch size\n",
    "# -Full epoc currently is 1:15h\n",
    "# TPUs\n",
    "# -After figuring out TPUs, 13min epoch is achieved!\n",
    "\n",
    "# -execution adjustments, batch siez, caching.\n",
    "# -\n",
    "\n",
    "# Need to figure out what's going on with the PAF stages, all losses seem the same.\n",
    "# 1.maybe the same loss function is applied to all.\n",
    "# check names of stages, and maybe have different losses for each.\n",
    "# check model outputs and graphs\n",
    "# 2.maybe the dataset is all zeros for pafs, or the limbs are too small,\n",
    "# optionally disable feedforward links\n",
    "# 3.another option is that it's ok\n",
    "\n",
    "# observations\n",
    "# *when the gradients explode, the PAF stages losses are very different:\n",
    "# loss: 6034892169562175.0000 - stage1paf_output_loss: 2.7209 - stage2paf_output_loss: 3066.9797 - stage3paf_output_loss: 1367401.7500 - stage4paf_output_loss: 648865408.0000\n",
    "\n",
    "# explosing gradients seem to have something with reloading checkpoints.\n",
    "# if true I should open an issue, on econd look it might just be the learning rate.\n",
    "\n",
    "# Learning rate\n",
    "# 0.0001 works fine \n",
    "# 0.003 explodes\n",
    "# 0.002 is too much as well\n",
    "\n",
    "\n",
    "# After training, the PAF stages dont seem to train at all. the converge to a zero minima.\\n\",\n",
    "# to solve the issue two solutions are implmented.\\n\",\n",
    "# 1.Implement the masking feature in the loss function, for the data in COCO dataset\\n\",\n",
    "# 2.Instead of using the paper's approach of hard areas for the PAFs, using a gaussian distribution for the lengths of the joints.\\n\",\n",
    "# 3.Adding a start and end gaussian distrubtion to the joint vectors (the beginning and end of the joints), this should improve handling of smaller joints like nose-eye and eye-ear.\\n\",\n",
    "# 4.Adding a neck kpt and changing the nose connection to this point. this reflection of anatomy should work better.\"\n",
    "\n",
    "\n",
    "# ERROR,\n",
    "# loss is NaN after 5 batch (128) training.\n",
    "# -lowering the LR didn't help\n",
    "# -switching to batch size one to find if is related to data.\n",
    "# -running 1 batch size, the training is going on fine, before crashing.\n",
    "# -error occurs at item 815., disabled shuffle\n",
    "# -retry, happens at 795\n",
    "# -interesting bug, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "michael_zl_prime\n",
      "                  Credentialed Accounts\n",
      "ACTIVE  ACCOUNT\n",
      "        312164605303-compute@developer.gserviceaccount.com\n",
      "*       michael.zl.prime@gmail.com\n",
      "\n",
      "To set the active account, run:\n",
      "    $ gcloud config set account `ACCOUNT`\n",
      "\n",
      "Python running from: /usr\n",
      "Current working dir /home/michael_zl_prime/project_drive/project\n"
     ]
    }
   ],
   "source": [
    "#verify user\n",
    "!whoami\n",
    "#verify user account, if running on google cloud, otherwise ignore\n",
    "!gcloud auth list\n",
    "#which enciorment/virtualenv running in\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"Python running from:\",sys.prefix)\n",
    "print(\"Current working dir\",os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#start tensor board\n",
    "# must run \n",
    "#/usr/local/bin/tensorboard serve --logdir gs://dl_training_results/tensorboard --port 8889 --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# get TF logger\n",
    "log = logging.getLogger('tensorflow')\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create file handler which logs even debug messages\n",
    "fh = logging.FileHandler('/tmp/tensorflow.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "log.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.1.0-dev20191203\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow version:\",tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results bucket connectivity\n",
      "Copying file:///tmp/test [Content-Type=application/octet-stream]...\n",
      "/ [1 files][    0.0 B/    0.0 B]                                                \n",
      "Operation completed over 1 objects.                                              \n",
      "Removing gs://dl_training_results/tensorboard//test...\n",
      "/ [1 objects]                                                                   \n",
      "Operation completed over 1 objects.                                              \n",
      "Copying file:///tmp/test [Content-Type=application/octet-stream]...\n",
      "/ [1 files][    0.0 B/    0.0 B]                                                \n",
      "Operation completed over 1 objects.                                              \n",
      "Removing gs://dl_training_results/checkpoints//test...\n",
      "/ [1 objects]                                                                   \n",
      "Operation completed over 1 objects.                                              \n",
      "Testing dataset bucket connectivity\n",
      "gs://datasets_bucket_a/training-001.tfrecords\n",
      "gs://datasets_bucket_a/training-002.tfrecords\n",
      "gs://datasets_bucket_a/training-003.tfrecords\n",
      "gs://datasets_bucket_a/training-004.tfrecords\n",
      "Testing TPU connectivity\n",
      "\n",
      "Starting Nmap 7.40 ( https://nmap.org ) at 2019-12-08 02:19 UTC\n",
      "Nmap scan report for 10.0.3.2\n",
      "Host is up (0.0091s latency).\n",
      "PORT     STATE SERVICE\n",
      "8470/tcp open  cisco-avp\n",
      "\n",
      "Nmap done: 1 IP address (1 host up) scanned in 0.05 seconds\n",
      "Trying to connect to a TPU node\n",
      "\n",
      "!!!MAKE SURE THE TPU ADDRESS IS CORRECT!!\n",
      "1.ip must be correct\n",
      "2.tpu must be turned on\n",
      "3.version must be 'nightly-2.x'\n",
      "4.tpu must be reachable (check with gce netowrking/connectivity test)\n",
      "if not this will hang!\n",
      "\n",
      "Trying to connect to: grpc://10.0.3.2:8470\n",
      "INFO:tensorflow:Initializing the TPU system: grpc://10.0.3.2:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.0.3.2:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "if TPU_MODE:\n",
    "    print(\"Testing results bucket connectivity\")\n",
    "    !touch /tmp/test\n",
    "    !gsutil cp /tmp/test {TENSORBOARD_PATH}/test\n",
    "    !gsutil rm {TENSORBOARD_PATH}/test\n",
    "    !gsutil cp /tmp/test {CHECKPOINTS_PATH}/test\n",
    "    !gsutil rm {CHECKPOINTS_PATH}/test\n",
    "    print(\"Testing dataset bucket connectivity\")\n",
    "    !gsutil ls gs://{GCS_TFRECORDS_BUCKETNAME} | head -4\n",
    "    print(\"Testing TPU connectivity\")\n",
    "    !nmap -Pn -p8470 {TPU_IP}\n",
    "    import tpu_training.init_tpu as init_tpu\n",
    "    strategy,resolver=init_tpu.init_tpu() #This must be run before any imports!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import datetime\n",
    "import dataset_functions\n",
    "from models.six_stage_linear_model import ModelMaker\n",
    "import callbacks\n",
    "import dataset_builder\n",
    "import load_weights\n",
    "import loss_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving TFrecords in TPU_mode\n",
      "Found the following TFrecords:\n",
      " gs://datasets_bucket_a/training-001.tfrecords\n",
      "gs://datasets_bucket_a/training-002.tfrecords\n",
      "gs://datasets_bucket_a/training-003.tfrecords\n",
      "gs://datasets_bucket_a/training-004.tfrecords\n",
      "gs://datasets_bucket_a/training-005.tfrecords\n",
      "gs://datasets_bucket_a/training-006.tfrecords\n",
      "gs://datasets_bucket_a/training-007.tfrecords\n",
      "gs://datasets_bucket_a/training-008.tfrecords\n",
      "gs://datasets_bucket_a/training-009.tfrecords\n",
      "gs://datasets_bucket_a/training-010.tfrecords\n",
      "gs://datasets_bucket_a/training-011.tfrecords\n",
      "gs://datasets_bucket_a/training-012.tfrecords\n",
      "gs://datasets_bucket_a/training-013.tfrecords\n",
      "gs://datasets_bucket_a/training-014.tfrecords\n",
      "gs://datasets_bucket_a/training-015.tfrecords\n",
      "gs://datasets_bucket_a/training-016.tfrecords\n",
      "gs://datasets_bucket_a/training-017.tfrecords\n",
      "gs://datasets_bucket_a/training-018.tfrecords\n",
      "gs://datasets_bucket_a/training-019.tfrecords\n",
      "gs://datasets_bucket_a/training-020.tfrecords\n",
      "gs://datasets_bucket_a/training-021.tfrecords\n",
      "gs://datasets_bucket_a/training-022.tfrecords\n",
      "gs://datasets_bucket_a/training-023.tfrecords\n",
      "gs://datasets_bucket_a/training-024.tfrecords\n",
      "gs://datasets_bucket_a/training-025.tfrecords\n",
      "gs://datasets_bucket_a/training-026.tfrecords\n",
      "gs://datasets_bucket_a/training-027.tfrecords\n",
      "gs://datasets_bucket_a/training-028.tfrecords\n",
      "gs://datasets_bucket_a/training-029.tfrecords\n",
      "gs://datasets_bucket_a/training-030.tfrecords\n",
      "gs://datasets_bucket_a/training-031.tfrecords\n",
      "gs://datasets_bucket_a/training-032.tfrecords\n",
      "gs://datasets_bucket_a/training-033.tfrecords\n",
      "gs://datasets_bucket_a/training-034.tfrecords\n",
      "gs://datasets_bucket_a/training-035.tfrecords\n",
      "gs://datasets_bucket_a/training-036.tfrecords\n",
      "gs://datasets_bucket_a/training-037.tfrecords\n",
      "gs://datasets_bucket_a/training-038.tfrecords\n",
      "gs://datasets_bucket_a/training-039.tfrecords\n",
      "gs://datasets_bucket_a/training-040.tfrecords\n",
      "gs://datasets_bucket_a/training-041.tfrecords\n",
      "gs://datasets_bucket_a/training-042.tfrecords\n",
      "gs://datasets_bucket_a/training-043.tfrecords\n",
      "gs://datasets_bucket_a/training-044.tfrecords\n",
      "gs://datasets_bucket_a/training-045.tfrecords\n",
      "gs://datasets_bucket_a/training-046.tfrecords\n",
      "gs://datasets_bucket_a/training-047.tfrecords\n",
      "gs://datasets_bucket_a/training-048.tfrecords\n",
      "gs://datasets_bucket_a/training-049.tfrecords\n",
      "gs://datasets_bucket_a/training-050.tfrecords\n",
      "gs://datasets_bucket_a/training-051.tfrecords\n",
      "gs://datasets_bucket_a/training-052.tfrecords\n",
      "gs://datasets_bucket_a/training-053.tfrecords\n",
      "gs://datasets_bucket_a/training-054.tfrecords\n",
      "gs://datasets_bucket_a/training-055.tfrecords\n",
      "gs://datasets_bucket_a/training-056.tfrecords\n",
      "gs://datasets_bucket_a/training-057.tfrecords \n",
      " gs://datasets_bucket_a/validation-001.tfrecords\n",
      "gs://datasets_bucket_a/validation-002.tfrecords\n",
      "gs://datasets_bucket_a/validation-003.tfrecords\n",
      "Building training dataset\n",
      "Training dataset shape: <PrefetchDataset shapes: (((None, 368, 368, 3), (None, 46, 46, 1)), ((None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 19), (None, 46, 46, 19))), types: ((tf.float32, tf.float32), (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32))>\n",
      "Building validation dataset\n",
      "Validation dataset shape: <BatchDataset shapes: (((None, 368, 368, 3), (None, 46, 46, 1)), ((None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 35), (None, 46, 46, 19), (None, 46, 46, 19))), types: ((tf.float32, tf.float32), (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32))>\n"
     ]
    }
   ],
   "source": [
    "tfrecord_files_train,tfrecord_files_val=dataset_builder.get_tfrecord_filenames()\n",
    "print(\"Found the following TFrecords:\\n\",\"\\n\".join(tfrecord_files_train),\"\\n\", \"\\n\".join(tfrecord_files_val))\n",
    "\n",
    "print(\"Building training dataset\")\n",
    "dst=dataset_builder.build_training_ds(tfrecord_files_train)\n",
    "print(\"Training dataset shape:\",dst)\n",
    "print(\"Building validation dataset\")\n",
    "dsv=dataset_builder.build_validation_ds(tfrecord_files_val)\n",
    "print(\"Validation dataset shape:\",dsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Test element\n",
    "# dst_iter=iter(dst)\n",
    "# sample_elem=next(dst_iter)\n",
    "# print(\"Dataset shape:\",dst) #this should match the model input, and output stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#testing mask\n",
    "# sample_elem=next(dst_iter)\n",
    "# m=sample_elem[1][1][0,...,0]\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(m)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fit_callbacks=[\n",
    "    callbacks.checkpoints_callback\n",
    "    ,callbacks.tensorboard_callback\n",
    "    ,callbacks.learning_rate_scheduler_callback\n",
    "    ,callbacks.print_lr_callback\n",
    "    ,tf.keras.callbacks.TerminateOnNaN()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "#run to clean tensorboard results\n",
    "#!gsutil -m rm -r {TENSORBOARD_PATH}/*\n",
    "#!gsutil -m rm -r {CHECKPOINTS_PATH}/*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model\n",
    "## Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "checkpoint=None\n",
    "starting_epoch=0\n",
    "if ASK_FOR_CHECKPOINTS:\n",
    "    checkpoint,starting_epoch=load_weights.checkpoints_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if INCLUDE_MASK:\n",
    "    loss=loss_metrics.MaskedMeanSquaredError\n",
    "    loss_name=\"Masked_MSE\"\n",
    "else:\n",
    "    loss=tf.keras.losses.MeanSquaredError\n",
    "    loss_name=\"Regular_MSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_maker=ModelMaker() #must be outside scope to keep the graph clean\n",
    "tf.keras.backend.clear_session() #to clean to backaend from the imported model\n",
    "def define():\n",
    "    train_model,test_model=model_maker.create_models()\n",
    "    \n",
    "    #this must match the model output order\n",
    "    metrics=[\n",
    "          [tf.keras.metrics.MeanAbsoluteError()]\n",
    "         ,[tf.keras.metrics.MeanAbsoluteError()]\n",
    "         ,[tf.keras.metrics.MeanAbsoluteError()]\n",
    "         ,[tf.keras.metrics.MeanAbsoluteError()]\n",
    "         ,[#tf.keras.metrics.Recall(),\n",
    "                    tf.keras.metrics.MeanAbsoluteError()]    \n",
    "         ,[#tf.keras.metrics.Recall(),\n",
    "                    tf.keras.metrics.MeanAbsoluteError()]\n",
    "        ]\n",
    "    \n",
    "    train_model.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(BASE_LEARNING_RATE)                   \n",
    "                    ,loss=loss(name=loss_name)\n",
    "                    ,metrics=metrics                           \n",
    "                   )\n",
    "    return train_model,test_model\n",
    "\n",
    "if TPU_MODE:\n",
    "    with strategy.scope():\n",
    "        train_model,test_model=define()\n",
    "        if (checkpoint):\n",
    "            train_model.load_weights(checkpoint)\n",
    "else:\n",
    "    train_model,test_model=define()\n",
    "    if (checkpoint):\n",
    "        train_model.load_weights(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Actually training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 437 steps\n",
      "\n",
      "Learning rate for epoch 0 is 0.00019999999494757503\n",
      "Epoch 1/50\n",
      "  7/437 [..............................] - ETA: 31:34 - loss: 0.0843 - stage1paf_output_mask_loss: 0.0076 - stage2paf_output_mask_loss: 0.0078 - stage3paf_output_mask_loss: 0.0078 - stage4paf_output_mask_loss: 0.0079 - stage5heatmap_output_mask_loss: 0.0270 - stage6heatmap_output_mask_loss: 0.0263 - stage1paf_output_mask_mean_absolute_error: 0.0188 - stage2paf_output_mask_mean_absolute_error: 0.0176 - stage3paf_output_mask_mean_absolute_error: 0.0179 - stage4paf_output_mask_mean_absolute_error: 0.0175 - stage5heatmap_output_mask_mean_absolute_error: 0.0793 - stage6heatmap_output_mask_mean_absolute_error: 0.0790"
     ]
    }
   ],
   "source": [
    "train_history=train_model.fit(\n",
    "    dst\n",
    "    ,epochs=50\n",
    "    ,steps_per_epoch=STEPS_PER_EPOCH\n",
    "    ,validation_data=dsv\n",
    "    ,callbacks=fit_callbacks\n",
    "    ,initial_epoch=starting_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model_path=MODELS_PATH+datetime.datetime.now().strftime(\"%d(%a)%m%y-%H%M\")\n",
    "tf.keras.models.save_model(train_model,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shut down TPU\n",
    "!gcloud compute tpus stop node-1 --zone us-central1-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#v.show_pafs_kpts_img(img.numpy(),paf.numpy(),kpt.numpy(),1,1) #can be used to draw the tensor data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
